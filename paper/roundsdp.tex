\documentclass[a4paper,10pt]{article}

\textheight235mm
\textwidth160mm
\voffset-10mm
\hoffset-10mm
\parindent0cm
\parskip2mm
\usepackage{listings}
\def\lstlanguagefiles{defManOcaml.tex}
\lstset{language = Ocaml}
\newcommand{\code}[1]{\lstinline{#1}}
%\begin{lstlisting}\end{lstlisting}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{graphicx}  % for pdf, bitmapped graphics files
\usepackage{amsmath} % assumes amsmath package installed
\usepackage{amssymb}  % assumes amsmath package installed
\usepackage{color}
\usepackage{myalgo}
\let\proof\relax
\let\endproof\relax
\usepackage{amsthm}
\usepackage{hyperref}
\usepackage{subfigure}
\usepackage{enumerate}
\usepackage{caption}
\usepackage{multirow}
%\usepackage{natbib} %for bibliography via bibtex
%\usepackage{enumitem}
%\lstMakeShortInline{$}
\newcommand{\add}[1]{#1}
\newcommand{\del}[1]{\textcolor{gray}{#1}}

%\newcommand{\P}{\mathbb{P}}
%\newcommand{\Q}{\mathbb{Q}}
%\newcommand{\E}{\mathbb{E}}
\def\sizefig{0.35}
\def\sizesmallfig{0.30}
\def\sizetinyfig{0.24}
\newcommand{\setA}{\mathcal{A}} % Semialgebraic functions
\newcommand{\setD}{\mathcal{D}} % Dictionnary of Univariate transcendental functions
\newcommand{\setU}{\mathcal{U}} % Univariate functions := \setT \cup {sqrt, abs, power functions}
\newcommand{\suppf}[1]{\text{supp}(#1)}
\newcommand{\mons}[2]{\N_{#1}^{#2}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\F}{\mathbb{F}}
\newcommand{\Pcal}{\mathcal{P}}
\newcommand{\Pcalp}{\Pcal^{(p)}}
\newcommand{\Scal}{\mathcal{S}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\x}{\mathbf{x}}
\newcommand{\e}{\mathbf{e}}
\newcommand{\y}{\mathbf{y}}
\newcommand{\z}{\mathbf{z}}
\newcommand{\alphab}{\boldsymbol{\alpha}}
\newcommand{\epsilonb}{\boldsymbol{\epsilon}}
\newcommand{\deltab}{\boldsymbol{\delta}}
\renewcommand{\b}{\mathbf{b}}
\newcommand{\f}{\mathbf{f}} 
\newcommand{\Plam}{\P_{\lambda}}
\newcommand{\Plamp}{\P_{\lambda}^{(p)}}
\def\P{\mathbf{P}}
\def\Q{\mathbf{Q}}
\def\L{\mathbf{L}}
\def\D{\mathbf{D}}
\def\q{\mathbf{q}}
\newcommand{\M}{\mathbf{M}}
\def\m{\mathbf{m}}
\def\H{\mathbf{H}}
\def\h{\mathbf{h}}
\def\f{f}
\def\a{\mathbf{a}}
\def\m{\mathbf{m}}
\def\p{\mathbf{p}}
\def\S{\mathbf{S}}
\def\B{\mathbf{B}}
\def\E{\mathbf{E}}
\def\K{\mathbf{K}}
\def\Q{\mathbf{Q}}
\def\X{\mathbf{X}}
\def\Y{\mathbf{Y}}
\newcommand{\A}{\mathbf{A}}
\newcommand{\Shat}{\hat{\S}}
\newcommand{\Bb}{\mathbf{B}}
\newcommand{\flam}{{f}_{{\lambda}}}
\newcommand{\flamfun}[1]{f_{\lambda}(#1)}
\newcommand{\flamfunp}[2]{f^{(#2)}_{\lambda}(#1)}
\newcommand{\flamx}{\flamfun{\x}}
\newcommand{\flampx}{\flamfunp{\x}{p}}
\newcommand{\flamstar}{f^*({\lambda})}
\newcommand{\flamstarj}[1]{f_{#1}^*({\lambda})}
\newcommand{\Mcal}{\mathcal{M}}
\newcommand{\nsdp}{n_{\text{sdp}}}
\newcommand{\nan}{\text{NaN}}
\newcommand{\nlift}{n_{\text{lift}}}
\newcommand{\Slift}{\S_{\text{lift}}}
\newcommand{\mlift}{m_{\text{lift}}}
\newcommand{\dlift}{r_{\text{lift}}}
\newcommand{\msdp}{m_{\text{sdp}}}
\newcommand{\xlamstar}{{\x}^*({\lambda})}
\newcommand{\transpose}{\top}%\newcommand{\transpose}{\mathbf{\intercal}}
\DeclareMathOperator{\vol}{vol}
%\DeclareMathOperator{\op}{op}
\DeclareMathOperator{\conv}{conv}
\newcommand{\red}[1]{\textbf{{\color{red}#1}}}
%\newcommand{\brev}{\color{red}}
%\newcommand{\erev}{\color{black}}
\newcommand{\sthreefp}{\mathtt{s3fp}}
\newcommand{\realtofloat}{\mathtt{Real2Float}}
\newcommand{\hol}{\text{\sc Hol-light}}
\newcommand{\ocaml}{\mathtt{OCaml}}
\newcommand{\op}{\mathtt{op}}
\newcommand{\bop}{\mathtt{bop}}
\newcommand{\coq}{\mathtt{Coq}}
\newcommand{\rosa}{\mathtt{rosa}}
\newcommand{\sdpa}{\text{\sc sdpa}}
\newcommand{\fptaylor}{\mathtt{FPTaylor}}
\newcommand{\nlcertify}{\mathtt{NLCertify}}
%\newcommand{\II}{\mathbb{I}}
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}

\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{hypothesis}[theorem]{Assumption}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{assumption}[theorem]{Assumption}
\newtheorem{example}{Example}

\theoremstyle{remark}
\newtheorem{remark}{Remark}
%\newtheorem*{note}{Note}
\newtheorem{case}{Case}

\title{\bf Automated Precision Tuning using Semidefinite Programming}
\begin{document}
\author{Victor Magron$^{1}$ \and George Constantinides$^{1}$ \and Alastair Donaldson$^{2}$}

\footnotetext[1]{Circuits and Systems Group, Department of Electrical and Electronic Engineering,
Imperial College London, South Kensington Campus, London SW7 2AZ, UK.}
\footnotetext[2]{Multicore Programming Group, Department of Computing,
Imperial College London, South Kensington Campus, London SW7 2AZ, UK.}
%\date{Draft of \today}

\maketitle
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{abstract}
%real2float allows the user to write programs with real-valued semialgebraic programs.
%Semidefinite programming can provide precise information to automatically tune reconfigurable hardware (e.g. FPGA) to algorithm specifications.
\end{abstract}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{Keywords}
hardware precision tuning; round-off error; numerical accuracy; floating-point arithmetic; fixed-precision arithmetic; semidefinite programming; sums of squares; correlation sparsity pattern
\section{Introduction} %3p
\label{sec:intro}
%
Constructing numerical programs which output accurate computation turns out to be difficult, due to finite numerical precision of implementations such as floating-point or fixed-point representations. Finite-precision numbers induce rounding errors, whose range knowledge is required to fulfil safety criteria of critical programs, as typically arising in modern embedded systems such as aircraft controllers.
To obtain lower bounds over rounding errors, one can rely on testing approaches, such as meta-heuristic search~\cite{Borges12Test} or under-approximations tools (e.g.~$\sthreefp$~\cite{Chiang14s3fp}). Here, we are interested in handling efficiently the complementary over-approximation problem, namely to obtain tight upper bounds over the error. This problem boils down to finding tight abstractions of non-linearities while being able to bound the resulting approximations in a efficient way.  
%


The aim of this work is to provide such a framework to perform automated precision analysis of computer programs consisting of nonlinear operations. 
This framework can be applied in general for developing accurate numerical software, but appears to be particularly relevant while considering algorithms migration onto reconfigurable hardware (e.g. FPGAs). The advantage of architectures based on FPGAs is that they allow more flexible choices, rather than choosing either for IEEE standard single or double precision. Indeed, in this case, one benefits from a more flexible number representation while ensuring guaranteed bounds on the program output. 

For computer programs consisting of linear operations, automatic error analysis can be obtained with well-studied optimization techniques based on SAT/SMT solvers~\cite{Darulova14Popl,hgbk2012fmcad}, Taylor-interval methods~\cite{fptaylor15}, affine arithmetic~\cite{fluctuat} and linear programming relaxations~\cite{Boland10HGR}.
These techniques can also be applied in the nonlinear setting but do not fully take into account the correlations between program variables. Thus they may output coarse error bounds or perform analysis within a large amount of time.  Another drawback of these tools is that they usually provide no formal guarantees. At best, they can produce proof certificates but the formal verification of these certificates is computationally expensive. It follows from the fact that most of computation performed in the informal optimization procedure are redone inside the proof assistant.

For computer programs with nonlinear operations, guarantees can be provided with certified programming techniques.
Semidefinite programming (SDP) is relevant to a wide range of mathematical fields, including combinatorial optimization, control theory, matrix completion. In 2001, Lasserre introduced a hierarchy of SDP relaxations~\cite{Lasserre01moments} for approximating polynomial infima. Our method to bound the error is a decision procedure based on an specialized variant of Lasserre hierarchy. The procedure relies on SDP to provide sparse sum-of-squares decompositions of positive polynomials. This framework handles polynomial program analysis (involving the operations $+,\times,-$) but can be extended to the more general class of semialgebraic or transcendental programs (involving $\sqrtsign, /, \min, \max, \arctan, \exp$), following the approximation scheme described in~\cite{Magron15sdp}.

\subsection{Overview of our Method}
%
Consider the program implementing the following polynomial expression $f$:
\[ f(\x) := x_2 \times x_5 + x_3 \times x_6 - x_2 \times x_3  - x_5 \times x_6 + x_1 \times ( - x_1 +  x_2 +  x_3  - x_4 +  x_5 +  x_6) \enspace, \]
where the six variable vector $\x :=  (x_1, x_2, x_3, x_4, x_5, x_6)$ is the input of the program. Here, the set $\X$ of possible input values is taken as a product of closed intervals: $\X = [4.00, 6.36]^6$ but could be defined in general with a set of inequality constraints among the variables $x_1, \dots, x_6$. 

The polynomial expression $f$ is obtained by performing 15 basic operations (1 negation, 3 subtractions, 6 additions and 5 multiplications). 
When executing this program with a set of floating-point numbers $\hat{\x} :=  (\hat{x}_1, \hat{x}_2, \hat{x}_3, \hat{x}_4, \hat{x}_5, \hat{x}_6) \in \X$, one actually computes a floating-point result $\hat{f}$, where all operations $+, -, \times$ are replaced by the respectively associated floating-point operations $\oplus, \ominus, \otimes$. 
The results of these operations comply with IEEE 754 standard arithmetic~\cite{IEEE} (see relevant background in Section~\ref{sec:fpbackground}). For instance, one can write $\hat{x}_2 \otimes \hat{x}_5 =  (x_2 \times x_5) (1 + e_1)$, by introducing an error variable $e_1$ such that $-\epsilon \leq e_1 \leq \epsilon$, where the bound $\epsilon$ is the machine precision (e.g.~$2^{-24}$ for single precision). One would like to bound the absolute rounding error $|r(\x, \e)| := | \hat{f}(\x, \e) - f (\x) |$ over  all possible input variables $\x \in \X$ and error variables $e_1, \dots, e_{15} \in [-\epsilon, \epsilon]$. Let define $\E := [-\epsilon, \epsilon]^{15}$ and $\K := \X \times \E$, then note that our bound problem can be cast as a nonlinear optimization problem:
%
\begin{equation}
\label{eq:roptim}
r^\star := \max_{(\x, \e) \in \K} | r(\x, \e) | = \max \{-\min_{(\x, \e) \in \K} r(\x, \e), \max_{(\x, \e) \in \K} r(\x,\e)\} \enspace. 
\end{equation}
%
One can directly try to solve these two polynomial optimization problems using classical SDP relaxations~\cite{Lasserre01moments}.
As in~\cite{fptaylor15}, one can also decompose the error term $r$ as the sum of a term $l(\x,\e)$, which is linear w.r.t.~$\e$, and a nonlinear term $h(\x,\e) := r(\x,\e) - l(\x,\e)$. Then the triangular inequality yields:
%
\begin{equation}
\label{eq:lhoptim} 
r^\star \leq \max_{(\x, \e) \in \K} |l(\x, \e)| + \max_{(\x, \e) \in \K} |h(\x, \e)| \enspace. 
\end{equation}
%
It follows that $l(\x,\e) = x_2 x_5 e_1 + x_3 x_6 e_2 +  (x_2 x_5 + x_3 x_6) e_3 + \dots + f(\x) e_{15} = \sum_{i=1}^{15} s_i(\x) e_i$. The {\em Symbolic Taylor expansions} method~\cite{fptaylor15} consists in using Taylor-interval optimization to compute a rigorous interval enclosure of each polynomial $s_i$, $i = 1,\dots,15$, over $\X$ and finally obtain an upper bound of $|l| + |h|$ over $\K$.
% over $\X \times \E$
\begin{itemize}
\item A direct attempt to solve the two polynomial problems occurring in~\eqref{eq:roptim} fails as the SDP solver (in our case $\sdpa$~\cite{sdpa7}) runs out of memory. 
\item Using sparse semidefinite optimization (derived from~\cite{Las06SparseSOS}) to bound $l$ and basic interval arithmetic to bound $h$, one obtains an upper bound of $789 \epsilon$ for $|l| + |h|$ over $\K$. This computation is performed in less than one second on a modern computer.
\item Using basic interval arithmetic, one obtains more quickly (about 17 times less CPU) a coarser bound of $2023 \epsilon$. 
\item Symbolic Taylor expansions provide an intermediate bound of $936 \epsilon$ but it takes about 16.3 times more CPU than our method. 
\item Finally, our bound is also obtained with the $\rosa$ tool~\cite{Darulova14Popl} but it takes about 4.6 times more CPU.
\end{itemize}

\if{
For instance, given a four variable vector $\x := (x_1, x_2, x_3, x_4)$, consider the  determinant computation of the $2 \times 2$ matrix  $\begin{pmatrix}
x_1 & x_2 \\
x_3 & x_4 \\
\end{pmatrix}$.
%
Then, the polynomial $f$ which represents the absolute floating-point error is given by $f(\x, \epsilonb) := [x_1 x_4 (1 + \epsilon_1) - x_3 x_2 (1 + \epsilon_2)] (1 + \epsilon_3) - x_1 x_4 + x_3 x_2$.
This polynomial $f$ is already of degree 3 and involves 7 variables.
}\fi




\subsection{Contributions}
Our key contributions can be summarized as follows:
\begin{itemize}
\item We present an optimization algorithm providing certified over-approximations for round-off errors of nonlinear programs. This algorithm is based on sparse sums of squares programming. By comparison with other methods, our algorithm allows to obtain tighter upper bounds, while overcoming scalability and numerical issues inherent to SDP solvers. We also propose some extensions of our algorithm to programs involving non-polynomial components, including either semialgebraic or transcendental operations (e.g. $/, \sqrtsign, \arctan, \exp$), as well as conditionals.
%\item 
\item Our framework is fully implemented in the $\realtofloat$ tool.  Among several features, the tool can optionally perform formal verification of round-off error bounds for polynomial programs, inside the $\coq$ proof assistant~\cite{CoqProofAssistant}. The last software release of $\realtofloat$ provides $\ocaml$~\cite{OCaml} and $\coq$ libraries and is freely available from 
\begin{center}
\url{https://github.com/afd/real2float}.
\end{center}
%
Our implementation tool is built in top of the $\nlcertify$ verification system~\cite{icms14}. Precision and efficiency of the tool are evaluated on several benchmarks coming from the existing literature. Numerical experiments demonstrate that our method competes well with recent approaches relying on Taylor-interval approximations~\cite{Darulova14Popl} or combining SMT solvers with affine arithmetic~\cite{Darulova14Popl}.
\end{itemize}
%


The paper is organized as follows.
%
In Section~\ref{sec:background}, we first recall mandatory background on rounding errors due to finite precision arithmetic (Section~\ref{sec:fpbackground}). Then we explain how to perform certified nonlinear optimization based on semidefinite programming (Section~\ref{sec:sdpbackground}) and finally how to obtain formal bounds while checking the certificates inside the $\coq$ proof assistant (Section~\ref{sec:coqbackground}).
%
Section~\ref{sec:fpsdp} contains the main contribution of the paper, namely how to compute tight over-approximations for rounding errors of nonlinear programs with sparse semidefinite relaxations.
%
Finally, Section~\ref{sec:benchs} is devoted to the evaluation of our nonlinear verification tool $\realtofloat$ on benchmarks arising from control systems, optimization, physics and biology.

%Verifying simple linear algebra algorithms can be troublesome from the  computational point of view.
\section{Preliminaries}
\label{sec:background}

\subsection{Program Semantics and Floating-point Arithmetic}
\label{sec:fpbackground}
We adopt the standard practice~\cite{higham2002accuracy} to approximate a real float $x$ with its closest floating-point representation $\hat{x} = x (1 + e)$, with $|e|$ being less than the machine precision $\epsilon$. This model is only valid when neglecting both overflow and denormal range values.
The operator $\hat{\cdot}$ is called the rounding operator and can be selected among rounding to nearest, rounding toward zero (resp.~$\infty$).
The scientific notation of a binary (resp.~decimal) floating-point number $\hat{x}$ is a triple $(s, sig, exp)$ consisting of a sign bit $s$, a {\em significand} $sig$ and an {\em exponent} $exp$, so that its numerical evaluation yields $(-1)^{s} \times sig \times 2^{exp}$ (resp.~$(-1)^{s} \times sig \times 10^{exp}$). 

The value of $\epsilon$ actually gives the upper bound of the relative floating-point error and is equal to $2^{-m}$, where $m$ is called the {\em precision}, referring to the number of significand bits used. For single floating-point precision, one has $m = 24$. For double (resp.~quadruple) precision, one has $m = 53$ (resp.~$m=113$). Let define $\R$ the set of real numbers and $\F$ the set of binary floating-point numbers.
For each real-valued operation $\bop_\R \in \{+, -, \times, \slash \}$ complying with IEEE 754 standard arithmetic~\cite{IEEE}, the result of the corresponding floating-point operation $\bop_\F \in \{\oplus, \ominus, \otimes, \oslash \}$ satisfies the following:
\begin{equation}
\label{eq:roundbop}
\bop_\F \, (\hat{x}, \hat{y}) = \bop_\R \, (x, y) \, (1 + e) \enspace, \quad \mid e \mid \leq \epsilon = 2^{-m} \enspace.
\end{equation}
%
Other operations include special functions taken from a {\em dictionary} $\setD$, containing the unary functions
$\tan$, $\arctan$, $\cos$, $\arccos$, $\sin$, $\arcsin$, $\exp$, $\log$, $(\cdot)^{r}$ with $r\in \R\setminus\{0\}$. For each $f_\R \in \setD$, the corresponding floating-point evaluation satisfies 
\begin{equation}
\label{eq:roundtransc}
f_\F (\hat{x}) = f_\R (x) (1 + e) \enspace, \quad \mid e \mid \leq \epsilon (f_\R) \enspace.
\end{equation}
%$f_\F (\hat{x}) = f_\R (x) (1 + e)$, with $|e| \leq \epsilon (f_\R)$. 
The value of the relative error bound $\epsilon (f_\R)$ differs from the machine precision $\epsilon$ in Equation~\eqref{eq:roundbop} and has to be properly adjusted. We refer the interested reader to~\cite{VerifCADTransc} for relative error bound verification of transcendental functions (see also~\cite{VerifHOLTransc} for formalization in $\hol$).
%
%In a similar fashion, 
\paragraph{Program semantics}
%
We consider generic programs encoded in an ML-like language:
\begin{lstlisting}
let box_prog       $x_1 \dots x_n = [(a_1, b_1); \dots ; (a_n, b_n)]$;;
let obj_prog       $x_1 \dots x_n = [(f(\x), \epsilon_{\text{total}})]$;;
let cstr_prog      $x_1 \dots x_n = [g_1 (\x), \dots, g_k(\x)]$;;
let uncertain_prog $x_1 \dots x_n = [u_1, \dots, u_n]$;;
\end{lstlisting}
Here, the first line encodes interval constraints for input variables, namely $\x := (x_1, \dots, x_n) \in [a_1, b_1]\times \dots \times [a_n, b_n]$.
The second line provides the function $f(\x)$ as well as the total rounding error bound $\epsilon_{\text{total}}$.
Then, one encodes polynomial nonnegativity constraints over the input variables, namely $g_1(\x) \geq 0, \dots, g_k(\x) \geq 0$. Finally, the last line allows the user to specify a numerical constant $u_i$ to associate a given uncertainty to the variable $x_i$, for each $i= 1, \dots, n$.

The type of numerical constants is denoted by \code{C}. In our current implementation, the user can choose either 64 bits floating-point or arbitrary-size rational numbers. This type \code{C} is used for the terms $\epsilon_{\text{total}}$, $u_1, \dots, u_n$, $a_1, \dots, a_n$, $b_1, \dots, b_n$.

The inductive type of polynomial expressions with coefficients in \code{C} is \code{polC} defined as follows:
\begin{lstlisting}
type polC = | Pc of C | Px of positive | Pneg of polC
| Padd of polC * polC | Pmul of polC * polC | Psub of polC * polC
\end{lstlisting}
%
The constructor \code{Px} takes a positive integer as argument to represent either an input or local variable.
%The polynomial expressions $g_1(\x), \dots, g_k(\x)$ have this type \code{polC}.
The inductive type \code{nlexpr} of nonlinear expressions (such as $f(\x)$) is defined as follows:
\begin{lstlisting}
type nlexpr = | Pol of polC | Neg of nlexpr
| Add of nlexpr * nlexpr | Mul of nlexpr * nlexpr | Sub of nlexpr * nlexpr 
| Div of nlexpr * nlexpr | Sqrt of nlexpr 
| Transc of transc * nlexpr
| IfThenElse of polC * nlexpr * nlexpr
| Let of positive * nlexpr * nlexpr
\end{lstlisting}
%
The type \code{transc} corresponds to the dictionary $\setD$ of special functions. For instance, the term~\lstinline|Transc ($\exp$, $f(\x)$)| represents the program implementing $\exp(f(\x)$.
Given a polynomial expression $p$ and two nonlinear expressions $f$ and $g$, the term ~\lstinline|IfThenElse($p(\x)$, $f(\x)$, $g(\x)$)| represents the conditional program implementing~\lstinline|if ($p(\x) \leq 0$) $f (\x)$ else $g (\x)$|. The constructor \code{Let} allows to define local variables in an ML fashion, e.g.~\lstinline|let $t_1 = 331.4 + 0.6 T$ in $-t_1 v /(t_1 + u)^2$| (part of the Doppler program considered in Section~\ref{sec:benchs}).
%
%\end{lstlisting}
%

Finally, one obtains rounded nonlinear expressions using an inductive procedure~\lstinline|rounding : nlexpr $\to$ nlexpr|, defined accordingly to Equation~\eqref{eq:roundbop} and Equation~\eqref{eq:roundtransc}. When an uncertainty $u_i$ is specified for an input variable $x_i$, the corresponding rounded expression is given by $x_i \, (1 + e)$, with $\mid e \mid \, \leq u_i$.
%to when solving optimization problems involving maximal absolute rounding errors. allowing to consider a single error variable bounded using $(k + 1) \epsilon$, thus saving $(k - 1)$ error variables.

%One can exploit sparsity in a way similar to the one described in~\cite{Waki06SparseSOS,Las06SparseSOS} to handle high dimensional problems.
\subsection{Sparse semidefinite relaxations for polynomial optimization}
\label{sec:sdpbackground}
Here, we recall mandatory background about the method that we use to handle the optimization problem of Equation~\eqref{eq:roptim}, when the nonlinear function $r$ is a polynomial expression.

\paragraph{Sums of squares certificates and semidefinite programming}
We remind basic facts about generation of sums of squares certificates for polynomial optimization, using semidefinite programming.
Denote by $\R[\x]$ the vector space of polynomials and by $\R_{2 d}[\x]$ the restriction of $\R[\x]$ to polynomials of degree at most $2 d$. Let define the cone of sums of squares:
\begin{equation}
\label{eq:cone_sos}
\Sigma[\x] := \Bigl\{\sum_i q_i^2, \, \text{ with } q_i \in \R[\x] \Bigr\}\enspace,
\end{equation}
%
as well as its restriction $\Sigma_{2 d}[\x] := \Sigma[\x] \cap \R_{2 d}[\x]$ to polynomials of degree at most $2 d$. For instance, the following bivariate polynomial  $\sigma (\x) := 1 + (x_1^2 - x_2^2)^2$ lies in $\Sigma_4[\x] \subseteq \R_4[\x]$.

At some point, optimization methods based on sums of squares use the implication $p \in \Sigma[\x] \implies \forall \x \in \R^n, \, p(\x) \geq 0$, i.e. the inclusion of $\Sigma[\x]$ in the cone of nonnegative polynomials.

Given $r \in \R[\x]$, one considers the following polynomial minimization problem:
\begin{equation}
\label{eq:minpop}
r^*  :=  \inf_{\x \in \R^n} \, \{ r (\x) \, : \, \x \in \K \, \} \enspace,
\end{equation}
%
where the set of constraints $\K \subseteq \R^n$ is defined by
%
\[\K := \{ \x \in \R^{n} : g_1 (\x) \geq 0, \dots, g_k (\x) \geq 0\}\enspace,\]
for polynomial functions $g_1, \dots, g_k$. The set $\K$ is called a {\em basic semialgebraic} set. Membership to semialgebraic sets is ensured by satisfying conjunctions of polynomial nonnegativity constraints. 
%
\begin{remark}
\label{rk:arch}
 When the input variables satisfy interval constraints $\x \in [a_1, b_1] \times \dots \times [a_n, b_n]$ then one can easily show that there exists some integer $M > 0$ such that $M - \sum_{i=0}^n x_i^2 \geq 0$. 
In the sequel, one assumes that this nonnegativity constraint appears explicitly in the definition of $\K$. Such an assumption is mandatory to prove the convergence of semidefinite relaxations stated in Theorem~\ref{th:densesdp}.
\end{remark}
%
In general, the objective function $r$ and the set of constraints $\K$ can be nonconvex, which makes the resolution of Problem~\eqref{eq:minpop} difficult to solve in practice. 
Note that one can rewrite Problem~\eqref{eq:maxpop} as the equivalent maximization problem:
\begin{equation}
\label{eq:maxpop}
r^*  :=  \sup_{\x \in \R^n, \mu \in \R} \{ \, \mu \, : \, r (\x) - \mu \geq 0 \,, \ \x \in \K \, \} \,.
\end{equation}
%
The existence of a sums of squares decomposition $p = \sum_i q_i^2$ valid
over $\R^n$ is ensured by the existence of a symmetric real matrix $Q$, solution of the following linear matrix feasibility problem:
\begin{align}
\label{eq:sdp}
p(\x) = \m_d(\x)^\intercal \, \Q \, \m_d(\x) \,, \quad \forall \x \in \R^n, \,
\end{align}
%
where $\m_d(\x) := (1, x_1, \dots, x_n, x_1^2,x_1 x_2,\dots, x_n^d)$ and the matrix $\Q$ has only nonnegative eigenvalues. Such a matrix $\Q$ is called {\em semidefinite positive}. The vector $\m_d$ and matrix $\Q$ have both a size equal to $s_n^d := {n + d \choose d}$. Problem~\eqref{eq:sdp} can be handled with semidefinite programming (SDP) solvers, such as {\sc Mosek}~\cite{mosek} or {\sc SDPA}~\cite{sdpa7} (see~\cite{Vandenberghe94SDP} for specific background about SDP). Then, one computes the ``LDL'' decomposition $\Q = \L^\intercal \D \L$ (variant of the classical Cholesky decomposition), where $\L$ is a lower triangular matrix and $\D$ is a diagonal matrix. Finally, one obtains $p(\x) =  (\L \,
\m_d(\x))^\intercal \, \D \, (\L \, \m_d(\x)) = \sum_{i=0}^{s_n^d} q_i(\x)^2$. Such a decomposition is called a sums of squares (SOS) {\em certificate}.
%
\begin{example}
Let define $p(\x) := 1 + x_1^4 - 2 x_1^2 x_2^2 + x_2^4$. With $\m_2 (\x) = (1, x_1, x_2, x_1^2, x_1 x_2, x_2^2)$, one solves the linear matrix feasibility problem $p(\x) = \m_2 (\x)^\intercal \, \Q \, \m_2(\x)$. One can show that the solution writes $\Q = \L^\intercal \D \L$ for a $6 \times 6$ matrix $\L$ and a diagonal matrix $\D$ with entries $(1,0,0,1,0,0)$, yielding the SOS decomposition: $p(\x) = 1 + (x_1^2 - x_2^2)^2 =: \sigma(\x)$.
\end{example}
%
\paragraph{Dense SDP relaxations for polynomial optimization}
We first explain how to obtain tractable appoximations of Problem~\eqref{eq:maxpop}. Define $g_0 := 1$. The hierarchy of semidefinite relaxations developed by Lasserre~\cite{Lasserre01moments} provides lower bounds of $r^*$, through solving the following programs $(\P_d)$:
\[
(\P_d):\left\{			
\begin{array}{lll}
r_d^\star := & \sup_{\mu, \sigma_j} & \mu\enspace, \\			 
& \text{s.t.} & r (\x) - \mu = \sum\limits_{j = 0}^{k} \sigma_j(\x) g_j(\x)\enspace, \\
& & \mu\in \R \,, \quad \sigma_j \in \Sigma[\x] \,, \quad \deg \sigma_j g_j \leq  2 d, \quad j = 0,\dots,k \enspace.\\
\end{array} \right.
\]
%
The next theorem is a consequence of the assumption mentioned in Remark~\ref{rk:arch}.
\begin{theorem}[Lasserre~\cite{Lasserre01moments}]
\label{th:densesdp}
The sequence $(r_d^\star)_d$ is nondecreasing and converges to $r^\star$.
\end{theorem}
%
The size of the truncated SDP variables
grows polynomially with the SDP-relaxation order $d$.
Indeed, at fixed $n$, the relaxation $\P_d$ involves $O((2 d)^{n})$ SDP
variables and $(k + 1)$ linear matrix inequalities (LMIs) of size
$O(d^n)$. When $d$ increases, then more accurate lower bounds of $r^\star$ can be obtained, at an increasing computational cost.
At fixed $d$,  the relaxation $\P_d$ involves $O(n^{2d})$ SDP variables and $(d + 1)$ linear matrix inequalities (LMIs) of size
$O(n^{d})$.

There are several ways to decrease the size of the SDP problems. 
First, symmetries in SDP relaxations for polynomial optimization problems can be exploited to replace one SDP problem $\P_d$ by
several smaller SDPs~\cite{Riener2013SymmetricSDP}. Notice that it is possible only if the multivariate polynomials of the initial problem are invariant under the action of a finite subgroup $G$ of the group $GL_{n}(\R)$. 
%

\paragraph{Exploiting sparsity} Here we describe how to exploit the structured sparsity of the
problem to replace one SDP problem $\P_d$ by an SDP problem of
size $O (\kappa^ {2 d})$ where $\kappa$ is the average size
of the maximal cliques correlation pattern of the polynomial
variables (see~\cite{Waki06SparseSOS}). 

We note $\N^n$ the set of $n$-tuple of nonnegative integers. The support of a polynomial $r(\x) := \sum_{\alphab \in \N^n} r_{\alphab} \x^{\alphab}$ is defined as $\suppf{r} := \{ \, \alphab \in \N^n \, : \, r_{\alphab} \neq 0 \, \}$. For instance the support of $p(\x) := 1 + x_1^4 - 2 x_1^2 x_2^2 + x_2^4$ is $\suppf{p} = \{ \, (0,0), (4, 0), (2,2), (0,4) \, \}$.

Let $F_j$ be the index set of variables which are involved in the polynomial $g_j$, for each $j=1, \dots, k$.
The correlative sparsity is represented by the 
$n \times n$ correlative sparsity matrix (csp matrix) $\mathbf{R}$ defined by:
\begin{equation}
\label{eq:csp}
\mathbf{R}(i, j) := \left \{
\begin{array}{ll}
  1 & \text{ if }  i = j \enspace, \\
  1 & \text{ if }  \exists \alphab \in \suppf{f} \text{ such that } \alpha_i \geq 1 \text{ and } \alpha_j \geq 1\enspace, \\
  1 & \text{ if }  \exists k \in \{1, \dots, m\} \text{ such that } i \in F_k \text{ and } j \in F_k \enspace,\\
  0 & \text{otherwise .} 
\end{array} \right.
\end{equation}

We define the undirected csp graph $G(N, E)$ with:
 \[ N = \{\, 1, \dots, n \,\} \text{ and } E = \{ \, \{i, j\} : i, j \in N \,, \ i < j \,, \mathbf{R}(i, j) = 1 \, \} \enspace. \]
Then, let $C_1,\dots, C_l \subset N$ denote the maximal cliques of $G(N, E)$ and 
%and define the sets of supports: 
%\[\mons{d}{C_q} := \{ \alphab \in  \mons{d}{n} : \alpha_i = 0 \text{ if } i %\notin C_q \}, \ (q=1 ,\dots,l)\enspace. \]
 define $n_j := \#C_j$, for each $j=1 ,\dots,l$.

%We assume that the module $QM(\K)$ is archimedean and that there is some $M > 0$ such that $M - \sum_{i = 1}^{n} {x_i^2} \geq 0$. 
From the assumption of Remark~\ref{rk:arch}, one can add the $q$ redundant additional constraints:
\begin{equation}
\label{eq:assum_sos_sparse}
g_{m + q} := n_q M^2 - \sum_{i \in C_q} {x_i^2} \geq 0, \enspace q=1 ,\dots, l\enspace,
\end{equation}
set $m' = m + q$, define the compact semialgebraic set:
\[\K' := \{ \x \in \R^{n} : g_1 (\x) \geq 0, \dots, g_m' (\x) \geq 0\}\enspace,\]
and modify Problem~\eqref{eq:minpop} into the following optimization problem:
\begin{equation}
\label{eq:cons_pop_sparse}
f^*  :=  \inf_{\x \in \K'} f (\x)\enspace,
\end{equation}

For each $q=1 ,\dots,l$, we also define the cone of sums of squares of polynomials in $\R_d[\x, \mons{d}{C_q}], (q=1 ,\dots,l)$:
\begin{equation}
\label{eq:cone_sos_sparse}
\Sigma [\x, \mons{d}{C_q}] := \Bigl\{\sum_i q_i^2, \, \text{ with } q_i \in \R_d[\x, \mons{d}{C_q}] \Bigr\}\enspace.
\end{equation}

Notice that the sums of squares of polynomials that belong to $\Sigma [\x, \mons{d}{C_q}]$ only involve variables $x_i (i \in C_q)$. 

The following program is the sparse variant of the semidefinite program $(\P_k)$:
\[
(\P^{\text{sparse}}_k):\left\{			
\begin{array}{ll}
\sup_{\mu, \sigma_j} & \mu\enspace, \\			 
\text{s.t.} & f (\x) - \mu = \sum\limits_{j = 0}^{m'} \sigma_j(\x) g_j(\x)\enspace, \\
& \mu\in \R,\qquad \sigma_j \in \Sigma[\x, \mons{k - \lceil \omega_j / 2 \rceil }{F_j}], j = 1,\dots,m' \enspace,\\
& \sigma_0 \in \sum\limits_{q = 1}^l \Sigma [\x, \mons{k}{C_q}]\enspace.
\end{array} \right.
\]

The interested reader can find more details about the properties of these semidefinite relaxations in \cite{Waki06SparseSOS}. Since the cliques $C_1, \dots, C_l$ satisfy the running intersection property (see Definition~\ref{def:rip} below), the optimal values of $\P_d^{\text{sparse}}$ converge to the global minimum $f^*$, as a corollary of~\cite[Theorem 3.6]{Las06SparseSOS}.

\begin{definition}[Running Intersection Property]
\label{def:rip}
Let $q \in \N_0, I_1, \dots, I_q \subset \{1, \dots, n\}$. We say that $I_1, \dots, I_q$ satisfy the running intersection property if: 
\[ \forall i = 2, \dots, r, (\exists k < i, (I_i \cap \bigcup\limits_{j < i} I_j) \subset I_k)\enspace. \]
\end{definition}

We illustrate the benefits of these sparse semidefinite relaxations with the following example:
\begin{example}
\label{ex:sparse}
Consider the polynomial $f$ mentioned in Section~\ref{sec:intro}:
\[ f(\x) := x_1 ( - x_1 +  x_2 +  x_3  - x_4 +  x_5 +  x_6) 
+ x_2 x_5 + x_3 x_6 - x_2 x_3  - x_5 x_6\enspace. \] 
Here, $n = 6, d = 2, N = \{1,\dots, 6 \}$. The $6 \times 6$ correlative sparsity matrix $\mathbf{R}$ is:
\[
\mathbf{R} = 
\begin{pmatrix}
  1 & 1 & 1 & 1 & 1 & 1 \\
  1 & 1 & 1 & 0 & 1 & 0 \\
  1 & 1 & 1 & 0 & 1 & 1 \\
  1 & 0 & 0 & 1 & 0 & 0 \\
  1 & 1 & 1 & 0 & 1 & 1 \\
  1 & 0 & 1 & 0 & 1 & 1 
 \end{pmatrix}
\]
The csp graph $G$ associated to $\mathbf{R}$ is depicted in Figure~\ref{fig:csp_deltax}. The maximal cliques of $G$ are:
\[ C_1 := \{1, 4\}, C_2 := \{1, 2, 3, 5\}, C_3 := \{1, 3, 5, 6\}\enspace. \]

For instance, the set of supports associated with the maximal clique $C_1$ for monomials of degree at most 2 is:
\begin{align*}
\mons{2}{C_1} := & \{ (0, 0, 0, 0, 0, 0), (1, 0, 0, 0, 0, 0),  (0, 0, 0, 1, 0, 0)\enspace, \\
                &(2, 0, 0, 0, 0, 0), (1, 0, 0, 1, 0, 0), (0, 0, 0, 2, 0, 0) \}\enspace. 
\end{align*}

Now consider the second order semidefinite relaxation. In the case of the dense semidefinite relaxation, the number of SDP variables is $\#\mons{4}{6} = \binom{6 + 4}{4} = 210$ since we are considering sums of squares of degree at most 4.
Conversely, in the case of the sparse relaxation, this number is $\#( \mons{4}{C_1} \cup \mons{4}{C_2} \cup \mons{4}{C_3}) = 115 $. The dense moment matrix is a single block diagonal matrix of size 28, while the sparse moment matrix is a block diagonal matrix with a $6 \times 6$ block and two $15 \times 15$ blocks.


\begin{figure}[!ht]	
\begin{center}
\includegraphics[scale=0.7]{csp_deltax.pdf}
\caption{Correlative sparsity pattern graph for the variables of $f$}
\label{fig:csp_deltax}
\end{center}
\end{figure}
\end{example}


\subsection{Computer assisted proofs for polynomial optimization}
\label{sec:coqbackground}
The aim of this section is to briefly recall some fundamental notions
about the mechanisms of theorem proving within the $\coq$ proof
assistant. For more details on the $\coq$ system, we recommend the
documentation available in~\cite{bertot2004interactive}.


\begin{figure}[!ht]
\centering
\includegraphics{reflexion.pdf}
\caption{An illustration of computational reflection}	
\label{fig:reflexion}
\end{figure}

\section{Guaranteed Round-off Error Bounds using SDP Relaxations}
\label{sec:fpsdp}

\paragraph{Simplification of error term products}
We first need the following lemma.
\begin{lemma}
\label{th:redproduct}
Let $\epsilon < \frac{1}{k}$ and define $\gamma_k := \frac{k \epsilon}{1 - k \epsilon}$. Then, for all $e_1, \dots, e_k \in [-\epsilon, \epsilon]$, there exists $\theta_k$ such that ${\displaystyle \prod_{i=1}^k (1 + e_i) = 1 + \theta_k}$ and $\mid \theta_k \mid \leq \gamma_k$.
\end{lemma}
\begin{proof}
The result is a direct consequence of~\cite[Lemma 3.3]{higham2002accuracy}.
\end{proof}
Lemma~\ref{th:redproduct} implies that for any $k$ such that $\epsilon < \frac{1}{k}$, one has $\gamma_k \leq (k + 1) \epsilon$. Hence, one can derive safe over-approximations of the absolute rounding error while introducing only one variable $e_1$ (bounded by $(k + 1) \epsilon$) instead of $k$ error variables $e_1, \dots, e_k$ (bounded by $\epsilon$). The cost of solving the corresponding optimization problem can be significantly reduced but it yields coarser error bounds.

\subsection{Polynomial Programs}

\begin{figure}[!ht]
\begin{algorithmic}[1]                    
\Require real polynomial result $f(\x)$, floating-point polynomial result $\hat{f}(\x, \epsilonb)$, set of constraints $\X$ on $\x$, set of constraints $\B$ on $\epsilonb$, relaxation order $k$
\Ensure upper bound $r_k^*$ for the absolute error $\mid \hat{f} - f  \mid$ over $\K := \X \times \B$
\State Define the error polynomial $r(\x, \epsilonb) := \hat{f}(\x) - f(\x, \epsilonb) = \displaystyle\sum_{\alphab} r_{\alphab}(\epsilonb) \x^{\alphab}$
%\State For each $\alphab$, compute a lower (resp. upper) degree 1 polynomial bound $\underline{r_{\alphab}}(\epsilonb)$ (resp. $\overline{r_{\alphab}}(\epsilonb)$) of $r_{\alphab}(\epsilonb)$ over $\B$ 
\For {each $\alphab$}
\State Write $r_{\alphab}(\epsilonb) = l_{\alphab}(\epsilonb) + h_{\alphab}(\epsilonb)$, with $\deg l_{\alphab} = 1$ and $\deg h_{\alphab} > 1$
%\State compute a degree 1 under-approximation $\underline{r_{\alphab}}(\epsilonb)$ of $r_{\alphab}(\epsilonb)$ over $\B$
%\State compute a degree 1 over-approximation $\overline{r_{\alphab}}(\epsilonb)$ of $r_{\alphab}(\epsilonb)$ over $\B$
\EndFor
\State Define $h(\x, \epsilonb) := \displaystyle\sum_{\alphab} h_{\alphab}(\epsilonb) \x^{\alphab}$ and $l(\x,\epsilonb):= \displaystyle\sum_{\alphab} l_{\alphab}(\epsilonb) \x^{\alphab}$
\State Compute an upper bound $h^*$ of $h$ over $\K$ with interval arithmetic
\State Compute an upper bound $l_k^*$ of $l$ over $\K$ with sparse SOS optimization at relaxation order $k$ \label{line:boundq}
%\State For each $\alphab$, compute an interval enclosure $[\underline{r_{\alphab}}, \overline{r_{\alphab}}]$ of $r_{\alphab}(\epsilonb)$ over $\B$
%\State Define the product of closed intervals $\Y := \prod_{\alphab} [\underline{r_{\alphab}}, \overline{r_{\alphab}}]$
%\State Compute a lower (resp. upper) bound $\underline{q}$ (resp. $\overline{q}$) of $q(\x,\y):= \sum_{\alphab} \y_{\alphab} \x^{\alphab}$ over $\X \times \Y$ \label{line:boundq}
%\State Compute an interval enclosure $I := [\underline{q}, \overline{q}]$ of $q(\x,\y):= \sum_{\alphab} \y_{\alphab} \x^{\alphab}$ over $\X \times \Y$ \label{line:boundq}
\State \Return $r_k^* := h^* + l_k^*$
\end{algorithmic}
\caption{$\realtofloat$}
\label{alg:realtofloat}
\end{figure}
%Let define $h_{\alphab} (y_{\alphab}) := (\overline{r_{\alphab}} - y_{\alphab}) (y_{\alphab} - \underline{r_{\alphab}})$, for each $\alphab$.
To compute an upper bound of the polynomial $l$, one writes
\[ 
l(\x,\epsilonb):= \sum_{\alphab} l_{\alphab}(\epsilonb) \x^{\alphab} = \sum_{i=1}^{n^\B} \epsilon_i \, l_i(\x) \,.
\]
Then one uses the results of Section~\ref{sec:sdpbackground}, namely one solves the following optimization problem:
%
\begin{align}
\label{eq:primalexists}
l^*_k := \inf\limits_{\sigma_j, \sigma_{0 i}, \sigma_{i}} \quad & \mu \\			
\text{s.t.} \quad & \mu - l(\x, \epsilonb) = 
\sum_{j=0}^m \sigma_j(\x) g_j(\x) + 
\sum_{i=1}^{n^{\B}}\sigma_{0 i}(\x,\epsilon_i) +  \sum_{i=1}^{n^{\B}}\sigma_i(\x,\epsilon_i) g_i^{\B}(\epsilon_i)  \,, \
\forall \x, \forall \epsilonb \,, \notag\\
\quad & 
\deg \sigma_j g_j \leq 2 k \,, \ 
\deg \sigma_{0 i} \leq 2 k \,, \ 
\deg \sigma_i g_j \leq 2 k \,, \ 
\forall i = 1,\dots,n^{\B} \,, \ 
\forall j = 0,\dots,m \,. \notag
\end{align}

\paragraph{Convergence of the SDP hierarchy of error bounds}

\subsection{Non-polynomial Programs}

Let $f : \S_f \to \R$ defined by $f(\x) := x_1 + \frac{x_2}{1 + x_3}$, for all $\x \in \S_f$. \\
To perform semialgebraic optimization via sums of squares, one usually introduces a lifting variable  $z$ to represent the division. Let $\S_g := \{(\x,z) : \x \in \S_f\,, \ z (1 + x_3) = 1  \}$.
%One can compute an interval enclosure $I_z$ of $z := 1 + x_3$ over $\S_f$.
%Let $\S_g := \S_f \times I_z$.
Then, one has $f(\x) = g(\x,z) := x_1 + x_2 z$, for all $(\x,z) \in \S_g$. \\
Here things are slightly different as we need to take into account the floating point representation of the denominator.\\
The floating point representation of $f$ is $\hat{f}(\x,\epsilonb) := [x_1 + \frac{x_2}{(1 + x_3)(1 + \epsilon_3)}(1 + \epsilon_2)](1 + \epsilon_1)$, for all $\x \in \S_f, \epsilonb \in \B$.
Then, one introduces a second lifting variable $\hat{z}$ and 
\[\K_f := \{(\x, \epsilonb, \hat{z}) : \x \in \S_f \,, \ \epsilonb \in \B \,, \ \hat{z} (1 + x_3) (1 + \epsilon_3) = 1  \} \,. \]
One has $\hat{f}(\x,\epsilonb) = \hat{g}(\x,\epsilonb,\hat{z}) := [x_1 + x_2 \hat{z}(1+\epsilon_2) ](1+\epsilon_1)$, for all $(\x, \epsilonb, \hat{z}) \in \K_f$. 
To bound the error $\hat{f} -f$, one optimizes the function $\hat{g} - g$ over the set 
\[\K_g := \{(\x, z, \epsilonb, \hat{z}) : (\x,z) \in \S_g \,, \ \epsilonb \in \B \,, \ \hat{z} (1 + x_3) (1 + \epsilon_3) = 1  \} \,. \]
\\
Here, one can actually reduce the number of variables of the problem, noticing that $z = \hat{z} (1 + \epsilon_3)$. However, in general one can not write $z$ as a polynomial of $\hat{z}$, $\x$ and $\epsilonb$ and one needs to consider two lifting variables for each non-polynomial operation involved in $f$.
%
%\subsection{Transcendental programs}

\section{Implementation Benchmarks} %4p
\label{sec:benchs}
\begin{table}[!ht]
%\small
\begin{center}
\caption{Comparison of error bounds $r_k^*$ obtained with different methods}
\begin{tabular}{p{2.3cm}lccccc}
\hline
\multirow{2}{*}{Benchmark} & \multirow{2}{*}{precision} & \multirow{2}{*}{$\realtofloat$} & $\rosa$  & $\fptaylor$  &\multirow{2}{*}{IA} & \multirow{2}{*}{Simulated error}
\\
& & & \cite{Darulova14Popl} & \cite{fptaylor15} & & \\
\hline            
%\multirow{2}{*}{Method 1} & $n^{(1)}$/$m^{(1)}$ &  $40/30$ & $212/111$ & $1039/350 $ & $4211/915$ & $130768/1991$ & $40251/3822$ \\
\multirow{1}{*}{doppler1$\star$}
& (double) & $5.05\text{e--}05$ & $2.36\text{e--}06$ & $6.40\text{e--}07$ & $3.95\text{e+}02$ & $5.97\text{e--}07$\\
\multirow{1}{*}{doppler1}
& (double) & $2.85\text{e--}13$ & $4.97\text{e--}13$ & $1.57\text{e--}13$ & $3.95\text{e+}02$ & $7.11\text{e--}14$\\
\multirow{1}{*}{doppler2}
& (double) & $1.02\text{e--}12$ & $1.29\text{e--}12$ & $2.87\text{e--}13$ & $\nan$ & $1.14\text{e--}13$\\
\multirow{1}{*}{doppler3}
& (double) & $1.45\text{e--}13$ & $2.03\text{e--}13$ & $8.16\text{e--}14$ & $1.09\text{e+}02$ & $4.27\text{e--}14$\\
\multirow{1}{*}{rigidBody1}
& (double) & $3.55\text{e--}13$ & $5.08\text{e--}13$ & $3.87\text{e--}13$ & $3.55\text{e--}13$ & $2.28\text{e--}13$\\
\multirow{1}{*}{rigidBody2}
& (double) & $3.98\text{e--}11$ & $6.48\text{e--}11$ & $5.24\text{e--}11$ & $3.98\text{e--}11$ & $2.19\text{e--}11$\\
\multirow{1}{*}{verhulst}
& (double) & $3.40\text{e--}16$ & $6.82\text{e--}16$ & $3.50\text{e--}16$ & $7.86\text{e--}01$ & $2.23\text{e--}16$\\
\multirow{1}{*}{kepler1}
& (double) & $2.96\text{e--}13$ & ? & $4.49\text{e--}13$ & $1.67\text{e--}12$ & $5.\text{e--}14$\\
\hline
\multirow{2}{*}{sineTaylor}
& (double) & $5.08\text{e--}16$ & $9.57\text{e--}16$ & $6.71\text{e--}16$ & $9.39\text{e--}16$ & $4.45\text{e--}16$\\
& (float) & $2.73\text{e--}07$ & $1.03\text{e--}06$ & $3.51\text{e--}07$ & $5.07\text{e--}07$ & $1.79\text{e--}07$\\
%& (16bit) & $2.24\text{e--}3$ & $2.87\text{e--}4$ & ? & $1.55\text{e--}4$\\
\hline
\multirow{2}{*}{sineOrder3}
& (double) & $6.53\text{e--}16$ & $1.11\text{e--}15$ & $9.96\text{e--}16$ & $8.82\text{e--}16$ & $3.34\text{e--}16$\\
& (float) & $3.51\text{e--}07$ & $1.19\text{e--}06$ & $5.35\text{e--}07$ & $4.74\text{e--}07$ & $2.12\text{e--}07$\\
\hline
\multirow{2}{*}{sqroot}
& (double) & $7.56\text{e--}16$ & $8.41\text{e--}16$ & $7.87\text{e--}16$ & $8.48\text{e--}16$ & $4.45\text{e--}16$\\
& (float) & $4.06\text{e--}07$ & $9.03\text{e--}07$ & $4.23\text{e--}07$ & $4.56\text{e--}07$ & $2.45\text{e--}07$\\
%& (16bit) & $3.33\text{e--}3$ & $5.97\text{e--}4$ & ? & $1.58\text{e--}4$\\
\hline
\end{tabular}
\label{table:error}
\end{center}
\end{table}


However, $\fptaylor$ implements special cases to eliminate some error terms, for instance when $\op$ is the multiplication and one of the operands is a nonnegative power of two, then the error $e$ is set to zero. 

{\scriptsize
\begin{lstlisting}
procedure doppler1(u : real, v : real, T : real) returns (r : real) {
 assume (-100.0 <= u && u <= 100.0 && 20.0 <= v && v <= 20000.0 && -30.0 <= T && T <= 50.0);
  var t1 := 331.4 + 0.6 * T;
  r := -t1*v/((t1 + u)*(t1 + u));
}
\end{lstlisting}
}
{\scriptsize
\begin{lstlisting}
procedure doppler2(u : real, v : real, T : real) returns (r : real) {
 assume (-125.0 <= u && u <= 125.0 && 15.0 <= v && v <= 25000.0 && -40.0 <= T && T <= 60.0);
  var t1 := 331.4 + 0.6 * T;
  r := -t1*v/((t1 + u)*(t1 + u));
}
\end{lstlisting}
}
{\scriptsize
\begin{lstlisting}
procedure doppler3(u : real, v : real, T : real) returns (r : real) {
 assume (-30.0 <= u && u <= 120.0 && 320.0 <= v && v <= 20300.0 && -50.0 <= T && T <= 30.0);
  var t1 := 331.4 + 0.6 * T;
  r := -t1*v/((t1 + u)*(t1 + u));
}
\end{lstlisting}
}
{\scriptsize
\begin{lstlisting}
procedure rigidBody1(x1 : real, x2 : real, x3 : real) returns (r : real) {
 assume (-15.0 <= x1 && x1 <= 15.0 && -15.0 <= x2 && x2 <= 15.0 && -15.0 <= x3 && x3 <= 15.0);
  r := -x1*x2 - 2.0 * x2 * x3 - x1 - x3;
}
\end{lstlisting}
}
{\scriptsize
\begin{lstlisting}
procedure rigidBody2(x1 : real, x2 : real, x3 : real) returns (r : real) {
 assume (-15.0 <= x1 && x1 <= 15.0 && -15.0 <= x2 && x2 <= 15.0 && -15.0 <= x3 && x3 <= 15.0);
  r := 2.0*x1*x2*x3 + 3.0*x3*x3 - x2*x1*x2*x3 + 3.0*x3*x3 - x2;
}
\end{lstlisting}
}
{\scriptsize
\begin{lstlisting}
procedure sineTaylor(x : real) returns (r : real) {
 assume (-1.57079632679  <= x && x <= 1.57079632679);
    r := x - (x*x*x)/6.0 + (x*x*x*x*x)/120.0 - (x*x*x*x*x*x*x)/5040.0 ;
}
\end{lstlisting}
}
{\scriptsize
\begin{lstlisting}
procedure sineOrder3(x : real) returns (r : real) {
 assume (-2  <= x && x <= 2);
  r := 0.954929658551372 * x - 0.12900613773279798*x*x*x ;
}
\end{lstlisting}
}
{\scriptsize
\begin{lstlisting}
procedure sqroot(x : real) returns (r : real) {
 assume (0  <= x && x <= 1);
  r := 1.0 + 0.5*x - 0.125*x*x + 0.0625*x*x*x - 0.0390625*x*x*x*x;
}
\end{lstlisting}
}
\section{Related Works} % 1.5p
%
SMT solvers allow to analyze programs with various semantics or specifications but are limited for the manipulation of problems involving nonlinear arithmetics. When SAT/SMT solvers output proof witnesses, they can be formally rechecked inside the $\coq$ proof assistant~\cite{smtcoq}. While ensuring soundness, the efficiency of the procedure is not compromised due to tactics enjoying the mechanism of computational reflection.

\section{Conclusion and Prospectives} %0.5p
%
We have presented a verification framework to over-approximate round-off errors occurring while executing nonlinear programs implemented with finite precision.
The framework relies on semidefinite optimization, ensuring tight and certified approximations. Our approach extends to medium-size nonlinear problems, due to  automatic detection of the correlation sparsity pattern of input variables and round-off error variables.

\if{
In this work, we have proposed a methods to approximate polynomial images of basic compact semialgebraic sets, a numerical approximation alternative to exact methods in case where the latter are too computationally demanding. In its present form, this methodology is applicable to problems of modest size, except if some sparsity can be taken into account, as explained earlier. 
Therefore, to handle larger size problems, the methodology needs to be adapted. 
A topic of further investigation is to search for alternative positivity certificates, less demanding than the SOS certificates used in this paper but more efficient than the LP based certificates as defined in~\cite{Handelman1988,Vasilescu}. On the one hand, the latter are really appealing since they yield a hierarchy of LP relaxations (as opposed to semidefinite relaxations as in this paper). Moreover, today's LP solvers can handle huge size LP problems, which is far from being the case for semidefinite solvers. But, on the other hand, it has been shown in~\cite{lasserre2009moments} that generically finite convergence cannot occur for convex problems, except for the linear case.
}\fi
\if{
\section*{Acknowledgments}
This work was partly funded by the Engineering and Physical Sciences Research Council (EPSRC) Challenging Engineering Grant (EP/I020457/1).
% Thank Jean-Michel Muller and Nathalie Revol for suggestions using simplified formulas involving power of x
% Thank Tillmann Weisser and Cordian Riener for suggesting the use of Newton sums to break problem symmetries
% Thank Johan LÃ¶fberg for his help concerning Yalmip scripts tuning
}\fi
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\addtolength{\textheight}{-3cm}   % This command serves to balance the column lengths

                                  % on the last page of the document manually. It shortens
                                  % the textheight of the last page by a suitable amount.
                                  % This command does not take effect until the next page
                                  % so it should come on the page before the last. Make
                                  % sure that you do not shorten the textheight too much.
                                  

%
\bibliographystyle{alpha}                       
%\bibliographystyle{alpha}
\bibliography{roundsdp}%2.5p

\end{document}

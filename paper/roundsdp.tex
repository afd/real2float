%-----------------------------------------------------------------------------
%
%               Template for sigplanconf LaTeX Class
%
% Name:         sigplanconf-template.tex
%
% Purpose:      A template for sigplanconf.cls, which is a LaTeX 2e class
%               file for SIGPLAN conference proceedings.
%
% Guide:        Refer to "Author's Guide to the ACM SIGPLAN Class,"
%               sigplanconf-guide.pdf
%
% Author:       Paul C. Anagnostopoulos
%               Windfall Software
%               978 371-2316
%               paul@windfall.com
%
% Created:      15 February 2005
%
%-----------------------------------------------------------------------------


\documentclass[preprint,fleqn,nocopyrightspace]{sigplanconf}

% The following \documentclass options may be useful:

% preprint      Remove this option only once the paper is in final form.
% 10pt          To set in 10-point type instead of 9-point.
% 11pt          To set in 11-point type instead of 9-point.
% authoryear    To obtain author/year citation style instead of numeric.
\usepackage{listings}
\def\lstlanguagefiles{defManOcaml.tex}
\lstset{language = Ocaml}
\newcommand{\code}[1]{\lstinline{#1}}
%\begin{lstlisting}\end{lstlisting}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{graphicx}  % for pdf, bitmapped graphics files
\usepackage{amsmath} % assumes amsmath package installed
\usepackage{amssymb}  % assumes amsmath package installed
\usepackage{color}
\usepackage{enumitem}
\usepackage{myalgo}
\let\proof\relax
\let\endproof\relax
\usepackage{amsthm}
%\usepackage{ntheorem}
\usepackage{hyperref}
\usepackage{subfigure}
\usepackage{enumerate}
%\usepackage{caption}
\usepackage{multirow}
\usepackage{tikz, pgfplots}
\pgfplotsset{compat=1.8}
%\usepackage{natbib} %for bibliography via bibtex
%\usepackage{enumitem}
%\lstMakeShortInline{$}
\newcommand{\add}[1]{#1}
\newcommand{\del}[1]{\textcolor{gray}{#1}}

%\newcommand{\P}{\mathbb{P}}
%\newcommand{\Q}{\mathbb{Q}}
%\newcommand{\E}{\mathbb{E}}
\def\sizefig{0.35}
\def\sizesmallfig{0.30}
\def\sizetinyfig{0.24}
\newcommand{\setA}{\mathcal{A}} % Semialgebraic functions
\newcommand{\setD}{\mathcal{D}} % Dictionnary of Univariate transcendental functions
\newcommand{\setU}{\mathcal{U}} % Univariate functions := \setT \cup {sqrt, abs, power functions}
\newcommand{\suppf}[1]{\text{supp}(#1)}
\newcommand{\mons}[2]{\N_{#1}^{#2}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\F}{\mathbb{F}}
\newcommand{\Pcal}{\mathcal{P}}
\newcommand{\Pcalp}{\Pcal^{(p)}}
\newcommand{\Scal}{\mathcal{S}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\x}{\mathbf{x}}
\newcommand{\e}{\mathbf{e}}
\newcommand{\y}{\mathbf{y}}
\newcommand{\z}{\mathbf{z}}
\newcommand{\nbenchs}{23}
\newcommand{\alphab}{\boldsymbol{\alpha}}
\newcommand{\epsilonb}{\boldsymbol{\epsilon}}
\newcommand{\deltab}{\boldsymbol{\delta}}
\renewcommand{\b}{\mathbf{b}}
\newcommand{\f}{\mathbf{f}} 
\newcommand{\Plam}{\P_{\lambda}}
\newcommand{\Plamp}{\P_{\lambda}^{(p)}}
\def\P{\mathbf{P}}
\def\Q{\mathbf{Q}}
\def\L{\mathbf{L}}
\def\D{\mathbf{D}}
\def\q{\mathbf{q}}
\newcommand{\M}{\mathbf{M}}
\def\m{\mathbf{m}}
\def\H{\mathbf{H}}
\def\h{\mathbf{h}}
\def\f{f}
\def\a{\mathbf{a}}
\def\m{\mathbf{m}}
\def\p{\mathbf{p}}
\def\S{\mathbf{S}}
\def\B{\mathbf{B}}
\def\E{\mathbf{E}}
\def\K{\mathbf{K}}
\def\S{\mathbf{S}}
\def\Q{\mathbf{Q}}
\def\X{\mathbf{X}}
\def\Y{\mathbf{Y}}
\newcommand{\A}{\mathbf{A}}
\newcommand{\Shat}{\hat{\S}}
\newcommand{\Bb}{\mathbf{B}}
\newcommand{\flam}{{f}_{{\lambda}}}
\newcommand{\flamfun}[1]{f_{\lambda}(#1)}
\newcommand{\flamfunp}[2]{f^{(#2)}_{\lambda}(#1)}
\newcommand{\flamx}{\flamfun{\x}}
\newcommand{\flampx}{\flamfunp{\x}{p}}
\newcommand{\flamstar}{f^*({\lambda})}
\newcommand{\flamstarj}[1]{f_{#1}^*({\lambda})}
\newcommand{\Mcal}{\mathcal{M}}
\newcommand{\nsdp}{n_{\text{sdp}}}
\newcommand{\divzero}{\text{Div0}}
\newcommand{\nlift}{n_{\text{lift}}}
\newcommand{\Slift}{\S_{\text{lift}}}
\renewcommand{\prec}{\text{prec}}
\newcommand{\mlift}{m_{\text{lift}}}
\newcommand{\dlift}{r_{\text{lift}}}
\newcommand{\msdp}{m_{\text{sdp}}}
\newcommand{\xlamstar}{{\x}^*({\lambda})}
\newcommand{\Kpol}{\K_{\text{poly}}}
\newcommand{\transpose}{\top}%\newcommand{\transpose}{\mathbf{\intercal}}
\DeclareMathOperator{\vol}{vol}
%\DeclareMathOperator{\op}{op}
\DeclareMathOperator{\conv}{conv}
\newcommand{\red}[1]{\textbf{{\color{red}#1}}}
\newcommand{\iaboundfun}[2]{\mathtt{ia\_bound}(#1, #2)}
\newcommand{\iabound}{\mathtt{ia\_bound}}
\newcommand{\sdpboundfun}[3]{\mathtt{sdp\_bound}(#1, #2, #3)}
\newcommand{\sdpbound}{\mathtt{sdp\_bound}}
\newcommand{\boundfun}[7]{\mathtt{bound}(#1, #2, #3, #4, #5, #6, #7)}
\newcommand{\bound}{\mathtt{bound}}
\newcommand{\boundnlprogfun}[7]{\mathtt{bound\_nlprog}(#1, #2, #3, #4, #5, #6, #7)}
\newcommand{\boundnlprog}{\mathtt{bound\_nlprog}}
\newcommand{\sdppolyfun}[3]{\mathtt{sdp\_poly}(#1, #2, #3)}
\newcommand{\sdppoly}{\mathtt{sdp\_poly}}
\newcommand{\liftfun}[3]{\mathtt{lift}(#1, #2, #3)}
\newcommand{\lift}{\mathtt{lift}}
\newcommand{\poly}{_\text{poly}}
\newcommand{\sa}{_\text{sa}}
\newcommand{\sdpsafun}[3]{\mathtt{sdp\_sa}(#1, #2, #3)}
\newcommand{\sdpsa}{\mathtt{sdp\_sa}}
\newcommand{\sdptranscfun}[3]{\mathtt{sdp\_trancs}(#1, #2, #3)}
\newcommand{\sdptransc}{\mathtt{sdp\_transc}}
%\newcommand{\brev}{\color{red}}
%\newcommand{\erev}{\color{black}}
\newcommand{\sthreefp}{\mathtt{s3fp}}
\newcommand{\realtofloat}{\mathtt{Real2Float}}
\newcommand{\smtcoq}{\mathtt{smtcoq}}
\newcommand{\dreal}{\mathtt{dReal}}
\newcommand{\hol}{\text{\sc Hol-light}}
\newcommand{\op}{\mathtt{op}}
\newcommand{\bop}{\mathtt{bop}}
\newcommand{\coq}{\text{\sc Coq}}
\newcommand{\ocaml}{\text{\sc OCaml}}
\newcommand{\rosa}{\mathtt{Rosa}}
\newcommand{\sdpa}{\text{\sc Sdpa}}
\newcommand{\fptaylor}{\mathtt{FPTaylor}}
\newcommand{\nlcertify}{\mathtt{NLCertify}}
\makeatletter
\newcommand*{\circled}{\@ifstar\circledstar\circlednostar}
\newcommand*{\squared}{\@ifstar\squaredstar\squarednostar}
\makeatother

\newcommand*\circledstar[1]{%
  \tikz[baseline=(C.base)]
    \node[%
      fill,
      circle,
      minimum size=1.em,
      text=white,
%      font=\sffamily,
      inner sep=0.5pt
    ](C) {\texttt{#1}};%
}
\newcommand*\circlednostar[1]{%
  \tikz[baseline=(C.base)]
    \node[%
      draw,
      circle,
      minimum size=1.em,
%      font=\sffamily,
      inner sep=0.5pt
    ](C) {\texttt{#1}};%
}
\newcommand*\squaredstar[1]{%
  \tikz[baseline=(C.base)]
    \node[%
      fill,
      rectangle,
      minimum size=1.em,
      text=white,
%      font=\sffamily,
      inner sep=0.5pt
    ](C) {\texttt{#1}};%
}
\newcommand*\squarednostar[1]{%
  \tikz[baseline=(C.base)]
    \node[%
      draw,
      rectangle,
      minimum size=1.em,
%      font=\sffamily,
      inner sep=0.5pt
    ](C) {\texttt{#1}};%
}
%\newcommand{\II}{\mathbb{I}}
%\theoremstyle{plain}
%\newtheorem{thm}{Theorem}[section]
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}

\theoremstyle{plain}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{hypothesis}[theorem]{Assumption}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{assumption}[theorem]{Assumption}
\newtheorem{example}{Example}

%\theoremstyle{remark}
\newtheorem{remark}{Remark}
%\newtheorem*{note}{Note}
\newtheorem{case}{Case}




\begin{document}

\conferenceinfo{CONF 'yy}{Month d--d, 20yy, City, ST, Country} 
\copyrightyear{20yy} 
\copyrightdata{978-1-nnnn-nnnn-n/yy/mm} 
\doi{nnnnnnn.nnnnnnn}

% Uncomment one of the following two, if you are not going for the 
% traditional copyright transfer agreement.

%\exclusivelicense                % ACM gets exclusive license to publish, 
                                  % you retain copyright

%\permissiontopublish             % ACM gets nonexclusive license to publish
                                  % (paid open-access papers, 
                                  % short abstracts)

%\titlebanner{banner above paper title}        % These are ignored unless
%\preprintfooter{short description of paper}   % 'preprint' option specified.

%\title{Automated Precision Tuning using Semidefinite Programming}
\title{Certified Roundoff Error Bounds Using Semidefinite Programming}

%\subtitle{Subtitle Text, if any}

\authorinfo{}{}{}
%\authorinfo{Name2\and Name3}
%           {Affiliation2/3}
%           {Email2/3}

\maketitle

\begin{abstract}

Roundoff errors cannot be avoided when implementing numerical programs with finite precision.         
The ability to reason about rounding is especially important if one wants to explore a range of potential representations, for instance in the world of FPGAs. This problem becomes challenging when the program does not employ solely linear operations as non-linearities are inherent to many interesting computational problems in real-world applications. 

Existing solutions to reasoning are limited in presence of nonlinear correlations between variables, leading to either imprecise bounds or high analysis time. Furthermore, while it is easy to implement a straightforward method such as interval arithmetic, sophisticated techniques are less straightforward to implement in a formal setting. Thus there is a need for methods which output certificates that can be formally  validated inside a proof assistant.

We present a framework to provide upper bounds of absolute roundoff errors. This framework is based on optimization techniques employing  semidefinite programming and sums of squares certificates, which can be formally checked inside the Coq theorem prover.
Our tool covers a wide range of nonlinear programs, including polynomials and transcendental operations as well as conditional statements.                                                             We illustrate the efficiency and  precision of this tool on non-trivial programs coming from biology, optimization and space control.
\end{abstract}

\category{D.2.4}{Software Engineering}{Program Verification}


% general terms are not compulsory anymore, 
% you may leave them out
%\terms
%term1, term2

\keywords
hardware precision tuning; round-off error; numerical accuracy; floating-point arithmetic; fixed-precision arithmetic; semidefinite programming; sums of squares; correlation sparsity pattern; proof assistant; formal verification.
%\if{
\section{Introduction} %3p
\label{sec:intro}
%
Constructing numerical programs which perform accurate computation turns out to be difficult, due to finite numerical precision of implementations such as floating-point or fixed-point representations. Finite-precision numbers induce roundoff errors,  and knowledge of the range of these roundoff errors is required to fulfill safety criteria of critical programs, as typically arising in modern embedded systems such as aircraft controllers. Such a knowledge can be used in general for developing accurate numerical software, but appears to be particularly relevant while considering algorithms migration onto reconfigurable hardware (e.g. FPGAs). The advantage of architectures based on FPGAs is that they allow more flexible choices, rather than choosing either for IEEE standard single or double precision. Indeed, in this case, we benefit from a more flexible number representation while ensuring guaranteed bounds on the program output. 

To obtain lower bounds over roundoff errors, one can rely on testing approaches, such as meta-heuristic search~\cite{Borges12Test} or under-approximation tools (e.g.~$\sthreefp$~\cite{Chiang14s3fp}). Here, we are interested in handling efficiently the complementary over-approximation problem, namely to obtain precise upper bounds over the error. This problem boils down to finding tight abstractions of linearities or non-linearities while being able to bound the resulting approximations in an efficient way.  
%
For computer programs consisting of linear operations, automatic error analysis can be obtained with well-studied optimization techniques based on SAT/SMT solvers~\cite{hgbk2012fmcad}, affine arithmetic~\cite{fluctuat}. However, non-linear operations are key to many interesting computational problems arising in physics, biology, controller implementations and global optimization. 
Recently, two promising frameworks have been designed to provide upper bounds for roundoff errors of nonlinear programs. The corresponding algorithms rely on Taylor-interval methods~\cite{fptaylor15}, implemented in the $\fptaylor$ tool, and on combining SMT with affine arithmetic~\cite{Darulova14Popl}, implemented in the $\rosa$ real compiler.

The common drawback of these two frameworks is that they do not fully take into account the correlations between program variables. Thus they may output coarse error bounds or perform analysis within a large amount of time.  

While the $\rosa$ tool is based on theoretical results that should provide sounds over-approximations of error bounds, the tool does not produce formal proof certificates to allow independent soundness checking.  The complexity of the mathematics underlying techniques for nonlinear reasoning, and the intricacies associated with constructing an efficient implementation, are such that a means for independent formal validation of results is particularly desirable.  
%The $\rosa$ tool does not provide any formal guarantees. 
To the best of our knowledge, the $\fptaylor$ software is the only academic tool which can produce formal proof certificates. This is based on the framework developed in~\cite{SolovyevH13} to verify nonlinear inequalities in $\hol$~\cite{hollight} using Taylor-interval methods. However, most of computation performed in the informal optimization procedure end up being redone inside the $\hol$ proof assistant, yielding a formal verification which is computationally demanding.
%The formal verification of these certificates is computationally expensive. This follows from the fact that most of computation performed in the informal optimization procedure end up being redone inside the proof assistant (see for instance in~\cite{SolovyevH13} the formal verification of nonlinear inequalities in $\hol$~\cite{hollight} using Taylor-interval methods).

The aim of this work is to provide a formal framework to perform automated precision analysis of computer programs that manipulate finite-precision data using nonlinear operators. For such programs, guarantees can be provided with certified programming techniques.
Semidefinite programming (SDP) is relevant to a wide range of mathematical fields, including combinatorial optimization, control theory and matrix completion. In 2001, Lasserre introduced a hierarchy of SDP relaxations~\cite{Lasserre01moments} for approximating polynomial infima. Our method to bound the error is a decision procedure based on an specialized variant of Lasserre hierarchy~\cite{Las06SparseSOS}. The procedure relies on SDP to provide sparse sum-of-squares decompositions of nonnegative polynomials. Our framework handles polynomial program analysis (involving the operations $+,\times,-$) as well as extensions to the more general class of semialgebraic and transcendental programs (involving $\sqrtsign, /, \min, \max, \arctan, \exp$), following the approximation scheme described in~\cite{Magron15sdp}.
%
\paragraph{Overview of our Method}
%
We present an overview of our method and of the capabilities of related techniques, using an example.
Consider a program implementing the following polynomial expression $f$:
\begin{align*}
f(\x) := x_2 \times x_5 + x_3 \times x_6 - x_2 \times x_3  - x_5 \times x_6 \\
+ x_1 \times ( - x_1 +  x_2 +  x_3  - x_4 +  x_5 +  x_6) \,,
\end{align*}
%
where the six-variable vector $\x :=  (x_1, x_2, x_3, x_4, x_5, x_6)$ is the input of the program. For this example, the set $\X$ of possible input values is a product of closed intervals: $\X = [4.00, 6.36]^6$.
This function $f$ together with the set $X$ appear in many inequalities arising from the the proof of Kepler Conjecture~\cite{halesalgo}, yielding challenging global optimization problems.

%but could be defined in general with a set of inequality constraints among the variables $x_1, \dots, x_6$. 

The polynomial expression $f$ is obtained by performing 15 basic operations (1 negation, 3 subtractions, 6 additions and 5 multiplications). 
When executing this program with a set of floating-point numbers $\hat{\x} :=  (\hat{x}_1, \hat{x}_2, \hat{x}_3, \hat{x}_4, \hat{x}_5, \hat{x}_6) \in \X$, one actually computes a floating-point result $\hat{f}$, where all operations $+, -, \times$ are replaced by the respectively associated floating-point operations $\oplus, \ominus, \otimes$. 
The results of these operations comply with IEEE 754 standard arithmetic~\cite{IEEE} (see relevant background in Section~\ref{sec:fpbackground}). For instance, one can write $\hat{x}_2 \otimes \hat{x}_5 =  (x_2 \times x_5) (1 + e_1)$, by introducing an error variable $e_1$ such that $-\epsilon \leq e_1 \leq \epsilon$, where the bound $\epsilon$ is the machine precision (e.g.~$\epsilon = 2^{-24}$ for single precision). One would like to bound the absolute roundoff error $|r(\x, \e)| := | \hat{f}(\x, \e) - f (\x) |$ over  all possible input variables $\x \in \X$ and error variables $e_1, \dots, e_{15} \in [-\epsilon, \epsilon]$. Let us define $\E := [-\epsilon, \epsilon]^{15}$ and $\K := \X \times \E$, then our bound problem can be cast as finding the maximum $r^\star$ of $\mid r \mid$ over $\K$, yielding the following nonlinear optimization problem:
%
\begin{align}
\begin{split}
\label{eq:roptim}
r^\star := & \max_{(\x, \e) \in \K} | r(\x, \e) | \\
 = & \ \ \max \{-\min_{(\x, \e) \in \K} r(\x, \e), \max_{(\x, \e) \in \K} r(\x,\e)\} \enspace,
\end{split}
\end{align}
%
One can directly try to solve these two polynomial optimization problems using classical SDP relaxations~\cite{Lasserre01moments}.
As in~\cite{fptaylor15}, one can also decompose the error term $r$ as the sum of a term $l(\x,\e)$, which is affine w.r.t.~$\e$, and a nonlinear term $h(\x,\e) := r(\x,\e) - l(\x,\e)$. Then the triangular inequality yields:
%
\begin{equation}
\label{eq:lhoptim} 
r^\star \leq \max_{(\x, \e) \in \K} |l(\x, \e)| + \max_{(\x, \e) \in \K} |h(\x, \e)| \enspace. 
\end{equation}
%
It follows for this example that $l(\x,\e) = x_2 x_5 e_1 + x_3 x_6 e_2 +  (x_2 x_5 + x_3 x_6) e_3 + \dots + f(\x) e_{15} = \sum_{i=1}^{15} s_i(\x) e_i$. The {\em Symbolic Taylor expansions} method~\cite{fptaylor15} consists of using Taylor-interval optimization to compute a rigorous interval enclosure of each polynomial $s_i$, $i = 1,\dots,15$, over $\X$ and finally obtain an upper bound of $|l| + |h|$ over $\K$. Our method uses sparse semidefinite relaxations for polynomial optimization (derived from \cite{Las06SparseSOS}) to bound $l$ as well as basic interval arithmetic to bound $h$. The following results have been obtained on an Intel Core i5 CPU ($2.40\, $GHz). All execution times have been computed by averaging over five runs.
% over $\X \times \E$
\begin{itemize}[noitemsep,nolistsep]
\item A direct attempt to solve the two polynomial problems occurring in Equation~\eqref{eq:roptim} fails as the SDP solver (in our case $\sdpa$~\cite{sdpa7}) runs out of memory. 
\item Using our method implemented in the $\realtofloat$ tool, one obtains an upper bound of $788 \epsilon$ for $|l| + |h|$ over $\K$ in less than one second. This bound is provided together with a certificate which can be formally checked inside the $\coq$ proof assistant in $0.34$ seconds.
\item Using basic interval arithmetic, one obtains 17.1 times more quickly a coarser bound of $2023 \epsilon$. 
\item Symbolic Taylor expansions implemented in $\fptaylor$ \cite{fptaylor15} provide an intermediate bound of $937 \epsilon$ but 16.3 times slower than with our implementation. Formal verification of this bound inside the $\hol$ proof assistant takes $73.8$ seconds and is 217 times slower than proof checking with $\realtofloat$ inside $\coq$.
\item Finally, our bound is also obtained with the $\rosa$ real compiler~\cite{Darulova14Popl} but 4.6 times slower than with our implementation.
\end{itemize}
%
%real2float 0.93 fptaylor 15.1 coq 0.333  hol 73.771
\paragraph{Contributions}
%
Our key contributions can be summarized as follows:
\begin{itemize}[noitemsep,nolistsep]
\item We present an optimization algorithm providing certified over-approximations for round-off errors of nonlinear programs. This algorithm is based on sparse sums of squares programming~\cite{Las06SparseSOS}. By comparison with other methods, our algorithm allows us to obtain tighter upper bounds, while overcoming scalability and numerical issues inherent in SDP solvers~\cite{Todd01semidefiniteoptimization}. Our algorithm can currently handle  programs implementing polynomial functions, but also involving non-polynomial components, including either semialgebraic or transcendental operations (e.g. $/, \sqrtsign, \arctan, \exp$), as well as conditional statements.  Programs containing iterative or while loops are not currently supported.
%\item 
\item Our framework is fully implemented in the $\realtofloat$ tool.  Among several features, the tool can optionally perform formal verification of round-off error bounds for polynomial programs, inside the $\coq$ proof assistant~\cite{CoqProofAssistant}. The last software release of $\realtofloat$ provides $\ocaml$~\cite{OCaml} and $\coq$ libraries and is freely available\footnote{\url{forge.ocamlcore.org/frs/?group_id=351} (\textbf{not anonymized})}.
%\begin{center}
%
%\end{center}
%
Our implementation tool is built in top of the $\nlcertify$ verification system~\cite{icms14}. Precision and efficiency of the tool are evaluated on several benchmarks coming from the existing literature. Numerical experiments demonstrate that our method competes well with recent approaches relying on Taylor-interval approximations~\cite{fptaylor15} or combining SMT solvers with affine arithmetic~\cite{Darulova14Popl}.
\end{itemize}
%


The paper is organized as follows.
%
In Section~\ref{sec:background}, we recall mandatory background on roundoff errors due to finite precision arithmetic before describing our nonlinear program semantics (Section~\ref{sec:fpbackground}). Then we remind how to perform certified polynomial optimization based on semidefinite programming (Section~\ref{sec:sdpbackground}) and how to obtain formal bounds while checking the certificates inside the $\coq$ proof assistant (Section~\ref{sec:coqbackground}).
%
Section~\ref{sec:fpsdp} contains the main contribution of the paper, namely how to compute tight over-approximations for roundoff errors of nonlinear programs with sparse semidefinite relaxations.
%
Finally, Section~\ref{sec:benchs} is devoted to the evaluation of our nonlinear verification tool $\realtofloat$ on benchmarks arising from control systems, optimization, physics and biology.
%Verifying simple linear algebra algorithms can be troublesome from the  computational point of view.
%\vspace*{-0.2cm}
\section{Preliminaries}
\label{sec:background}

\subsection{Program Semantics and Floating-point Numbers}
\label{sec:fpbackground}
We adopt the standard practice~\cite{higham2002accuracy} to approximate a real number $x$ with its closest floating-point representation $\hat{x} = x (1 + e)$, with $|e|$ is less than the machine precision $\epsilon$. The validity of this model is ensured as we neglect both overflow and denormal range values.
The operator $\hat{\cdot}$ is called the rounding operator and can be selected among rounding to nearest, rounding toward zero (resp.~$\pm\infty$).
The scientific notation of a binary (resp.~decimal) floating-point number $\hat{x}$ is a triple $(s, sig, exp)$ consisting of a sign bit $s$, a {\em significand} $sig \in [1, 2)$ (resp.~$[1, 10)$) and an {\em exponent} $exp$, yielding numerical evaluation $(-1)^{s} \, sig \, 2^{exp}$ (resp.~$(-1)^{s} \, sig \, 10^{exp}$). 

The value of $\epsilon$ actually gives the upper bound of the relative floating-point error and is equal to $2^{-\prec}$, where $\prec$ is called the {\em precision}, referring to the number of significand bits used. For single precision floating-point, one has $\prec = 24$. For double (resp.~quadruple) precision, one has $\prec = 53$ (resp.~$\prec=113$). Let us define $\R$ the set of real numbers and $\F$ the set of binary floating-point numbers.
For each real-valued operation $\bop_\R \in \{+, -, \times, \slash \}$, the result of the corresponding floating-point operation $\bop_\F \in \{\oplus, \ominus, \otimes, \oslash \}$ satisfies the following when complying with IEEE 754 standard arithmetic~\cite{IEEE}:
\begin{equation}
\label{eq:roundbop}
\bop_\F \, (\hat{x}, \hat{y}) = \bop_\R \, (x, y) \, (1 + e) \enspace, \quad \mid e \mid \leq \epsilon = 2^{-\prec} \enspace.
\end{equation}
%
Other operations include special functions taken from a {\em dictionary} $\setD$, containing the unary functions
$\tan$, $\arctan$, $\cos$, $\arccos$, $\sin$, $\arcsin$, $\exp$, $\log$, $(\cdot)^{r}$ with $r\in \R\setminus\{0\}$. For $f_\R \in \setD$, the corresponding floating-point evaluation satisfies 
\begin{equation}
\label{eq:roundtransc}
f_\F (\hat{x}) = f_\R (x) (1 + e) \enspace, \quad \mid e \mid \leq \epsilon (f_\R) \enspace.
\end{equation}
%$f_\F (\hat{x}) = f_\R (x) (1 + e)$, with $|e| \leq \epsilon (f_\R)$. 
The value of the relative error bound $\epsilon (f_\R)$ differs from the machine precision $\epsilon$ in Equation~\eqref{eq:roundbop} and has to be properly adjusted. We refer the interested reader to~\cite{VerifCADTransc} for relative error bound verification of transcendental functions (see also~\cite{VerifHOLTransc} for formalization in $\hol$).
%
%In a similar fashion, 
%\vspace*{-0.2cm}
\paragraph{Program semantics}
%
We support conditional code without procedure calls nor loops. Despite these restrictions, we can consider a wide range of nonlinear programs while assuming that  important numerical calculations can be expressed in a loop-free manner. 
Our programs are encoded in an ML-like language:
% We consider generic programs encoded in an ML-like language:
\begin{lstlisting}
let box_prog    $x_1 \dots x_n = [(a_1, b_1); \dots ; (a_n, b_n)]$;;
let obj_prog    $x_1 \dots x_n = [(f(\x), \epsilon_{\realtofloat})]$;;
let cstr_prog   $x_1 \dots x_n = [g_1 (\x); \dots; g_k(\x)]$;;
let uncert_prog $x_1 \dots x_n = [u_1; \dots; u_n]$;;
\end{lstlisting}
Here, the first line encodes interval constraints for input variables, namely $\x := (x_1, \dots, x_n) \in [a_1, b_1]\times \dots \times [a_n, b_n]$.
The second line provides the function $f(\x)$ as well as the total roundoff error bound $\epsilon_{\realtofloat}$.
Then, one encodes polynomial nonnegativity constraints over the input variables, namely $g_1(\x) \geq 0, \dots, g_k(\x) \geq 0$. Finally, the last line allows the user to specify a numerical constant $u_i$ to associate a given uncertainty to the variable $x_i$, for each $i= 1, \dots, n$.

The type of numerical constants is denoted by \code{C}. In our current implementation, the user can choose either 64 bits floating-point or arbitrary-size rational numbers. This type \code{C} is used for the terms $\epsilon_{\realtofloat}$, $u_1, \dots, u_n$, $a_1, \dots, a_n$, $b_1, \dots, b_n$.
%
The inductive type of polynomial expressions with coefficients in \code{C} is \code{pExprC} defined as follows:
\begin{lstlisting}
type pexprC = | Pc of C | Px of positive 
| Psub of pexprC$\,$*$\,$pexprC | Pneg of pexprC 
| Padd of pexprC$\,$*$\,$pexprC 
| Pmul of pexprC$\,$*$\,$pexprC
\end{lstlisting}
%
The constructor \code{Px} takes a positive integer as argument to represent either an input or local variable.
%The polynomial expressions $g_1(\x), \dots, g_k(\x)$ have this type \code{pexprC}.
The inductive type \code{nlexpr} of nonlinear expressions (such as $f(\x)$) is defined as follows:
\begin{lstlisting}
type nlexpr = 
| Pol of pexprC | Neg of nlexpr
| Add of nlexpr$\,$*$\,$nlexpr 
| Mul of nlexpr$\,$*$\,$nlexpr 
| Sub of nlexpr$\,$*$\,$nlexpr 
| Div of nlexpr$\,$*$\,$nlexpr | Sqrt of nlexpr 
| Transc of transc$\,$*$\,$nlexpr
| IfThenElse of pexprC$\,$*$\,$nlexpr$\,$*$\,$nlexpr
| Let of positive$\,$*$\,$nlexpr$\,$*$\,$nlexpr
\end{lstlisting}
%
The type \code{transc} corresponds to the dictionary $\setD$ of special functions. For instance, the term~\lstinline|Transc ($\exp$, $f(\x)$)| represents the program implementing $\exp(f(\x))$.
Given a polynomial expression $p$ and two nonlinear expressions $f$ and $g$, the term ~\lstinline|IfThenElse($p(\x)$, $f(\x)$, $g(\x)$)| represents the conditional program implementing~\lstinline|if ($p(\x) \geq 0$) $f (\x)$ else $g (\x)$|. The constructor \code{Let} allows us to define local variables in an ML fashion, e.g.~\lstinline|let $t_1 = 331.4 + 0.6 * T$ in $-t_1 * v /((t_1 + u) * (t_1 + u))$| (part of the \textit{doppler1} program considered in Section~\ref{sec:benchs}).
%
%\end{lstlisting}
%

Finally, one obtains rounded nonlinear expressions using an inductive procedure~\lstinline|round : nlexpr $\to$ nlexpr|, defined accordingly to Equation~\eqref{eq:roundbop} and Equation~\eqref{eq:roundtransc}. When an uncertainty $u_i$ is specified for an input variable $x_i$, the corresponding rounded expression is given by $x_i \, (1 + e)$, with $\mid e \mid \, \leq u_i$.
%to when solving optimization problems involving maximal absolute rounding errors. allowing to consider a single error variable bounded using $(k + 1) \epsilon$, thus saving $(k - 1)$ error variables.

%One can exploit sparsity in a way similar to the one described in~\cite{Waki06SparseSOS,Las06SparseSOS} to handle high dimensional problems.
\subsection{SDP relaxations for polynomial optimization}
\label{sec:sdpbackground}
The sums of squares method involves approximation of polynomial inequality constraints by sums of squares (SOS) equality constraints. Here we recall mandatory background about SOS. We apply this method in Section~\ref{sec:fpsdp} to solve the problems of Equation~\eqref{eq:roptim} when the nonlinear function $r$ is a polynomial.
%Here, we recall mandatory background about the method that we use to handle the optimization problems of Equation~\eqref{eq:roptim}, when the nonlinear function $r$ is a polynomial.  We apply this method in Section~\ref{sec:fpsdp} to solve the problems of Equation~\eqref{eq:roptim}.
%Our main contribution, presented in Section~\ref{sec:fpsdp} is to apply this method to solve Problem~\eqref{eq:roptim}.  
%
\paragraph{Sums of squares certificates and SDP}
%
First we remind basic acts about generation of SOS certificates for polynomial optimization, using semidefinite programming.
Denote by $\R[\x]$ the vector space of polynomials and by $\R_{2 d}[\x]$ the restriction of $\R[\x]$ to polynomials of degree at most $2 d$. Let us define the set of SOS polynomials:
\begin{equation}
\label{eq:cone_sos}
\Sigma[\x] := \Bigl\{\sum_i q_i^2, \, \text{ with } q_i \in \R[\x] \Bigr\}\enspace,
\end{equation}
%
as well as its restriction $\Sigma_{2 d}[\x] := \Sigma[\x] \bigcap \R_{2 d}[\x]$ to polynomials of degree at most $2 d$. For instance, the following bivariate polynomial  $\sigma (\x) := 1 + (x_1^2 - x_2^2)^2$ lies in $\Sigma_4[\x] \subseteq \R_4[\x]$.

Optimization methods based on SOS use the implication $p \in \Sigma[\x] \implies \forall \x \in \R^n, \, p(\x) \geq 0$, i.e. the inclusion of $\Sigma[\x]$ in the set of nonnegative polynomials.
%
Given $r \in \R[\x]$, one considers the following polynomial minimization problem:
\begin{equation}
\label{eq:minpop}
r^*  :=  \inf_{\x \in \R^n} \, \{ \, r (\x) \, : \, \x \in \K \, \} \enspace,
\end{equation}
%
where the set of constraints $\K \subseteq \R^n$ is defined by
%
\[\K := \{ \x \in \R^{n} : g_1 (\x) \geq 0, \dots, g_k (\x) \geq 0\}\enspace,\]
for polynomial functions $g_1, \dots, g_k$. The set $\K$ is called a {\em basic semialgebraic} set. Membership to semialgebraic sets is ensured by satisfying conjunctions of polynomial nonnegativity constraints. 
%
\begin{remark}
\label{rk:arch}
 When the input variables satisfy interval constraints $\x \in [a_1, b_1] \times \dots \times [a_n, b_n]$ then one can easily show that there exists some integer $M > 0$ such that $M - \sum_{i=1}^n x_i^2 \geq 0$. 
In the sequel, we assume that this nonnegativity constraint appears explicitly in the definition of $\K$. Such an assumption is mandatory to prove the convergence of semidefinite relaxations recalled in Theorem~\ref{th:densesdp}.
\end{remark}
%
In general, the objective function $r$ and the set of constraints $\K$ can be nonconvex, which makes the resolution of Problem~\eqref{eq:minpop} difficult to solve in practice. 
One can rewrite Problem~\eqref{eq:minpop} as the equivalent maximization problem:
\begin{equation}
\label{eq:maxpop}
r^*  :=  \sup_{\x \in \R^n, \mu \in \R} \{ \, \mu \, : \, r (\x) - \mu \geq 0 \,, \ \x \in \K \, \} \,.
\end{equation}
%
Now we outline how to handle the nonnegativity constraint $r - \mu \geq 0$.
Given a nonnegative polynomial $p \in \R[\x]$, the existence of an SOS decomposition $p = \sum_i q_i^2$ valid
over $\R^n$ is ensured by the existence of a symmetric real matrix $Q$, a solution of the following linear matrix feasibility problem:
\begin{align}
\label{eq:sdp}
p(\x) = \m_d(\x)^\intercal \, \Q \, \m_d(\x) \,, \quad \forall \x \in \R^n, \,
\end{align}
%
where $\m_d(\x) := (1, x_1, \dots, x_n, x_1^2,x_1 x_2,\dots, x_n^d)$ and the matrix $\Q$ has only nonnegative eigenvalues. Such a matrix $\Q$ is called {\em positive semidefinite}. The vector $\m_d$ (resp.~matrix $\Q$) has a size (resp.~dimension) equal to $s_n^d := \binom{n + d}{d}$. Problem~\eqref{eq:sdp} can be handled with semidefinite programming (SDP) solvers, such as {\sc Mosek}~\cite{mosek} or {\sc SDPA}~\cite{sdpa7} (see~\cite{Vandenberghe94SDP} for specific background about SDP). Then, one computes the ``LDL'' decomposition $\Q = \L^\intercal \D \L$ (variant of the classical Cholesky decomposition), where $\L$ is a lower triangular matrix and $\D$ is a diagonal matrix. Finally, one obtains $p(\x) =  (\L \,
\m_d(\x))^\intercal \, \D \, (\L \, \m_d(\x)) = \sum_{i=0}^{s_n^d} q_i(\x)^2$. Such a decomposition is called a sums of squares (SOS) {\em certificate}.
%
\begin{example}
\label{ex:sdp}
Let us define $p(\x) := \frac{1}{4} + x_1^4 - 2 x_1^2 x_2^2 + x_2^4$. With $\m_2 (\x) = (1, x_1, x_2, x_1^2, x_1 x_2, x_2^2)$, one solves the linear matrix feasibility problem $p(\x) = \m_2 (\x)^\intercal \, \Q \, \m_2(\x)$. One can show that the solution writes $\Q = \L^\intercal \D \L$ for a $6 \times 6$ matrix $\L$ and a diagonal matrix $\D$ with entries $(\frac{1}{2},0,0,1,0,0)$, yielding the SOS decomposition: $p(\x) = (\frac{1}{2})^2 + (x_1^2 - x_2^2)^2 =: \sigma(\x)$. It is enough to prove that $p$ is nonnegative.
\end{example}
%
\paragraph{Dense SDP relaxations for polynomial optimization}
%
In order to solve our goal problem (Problem~\eqref{eq:roptim}), we are trying to solve Problem~\eqref{eq:minpop}, recast as
Problem~\eqref{eq:maxpop}. We first explain how to obtain tractable approximations of this difficult problem. Define $g_0 := 1$. The hierarchy of SDP relaxations developed by Lasserre \cite{Lasserre01moments} provides lower bounds of $r^*$, through solving the  optimization problems $(\P_d)$:
\[
(\P_d):\left\{			
\begin{array}{rlr}
p_d^\star := \sup\limits_{\sigma_j, \mu} & \mu \enspace, \\			 
\text{s.t.} & \forall \x \,,\  r (\x) - \mu = \sum_{j = 0}^{k} \sigma_j(\x) g_j(\x) \,,  \\
\\
& \mu\in \R \,, \sigma_j \in \Sigma[\x] \,, \quad \  \quad  \ j = 0,\dots,k \,, \\
\\
& \deg (\sigma_j g_j) \leq  2 d,             \quad \ \,  \qquad  j = 0,\dots,k \,.\\
\end{array} \right.
\]
%
The next theorem is a consequence of the assumption mentioned in Remark~\ref{rk:arch}.
\begin{theorem}[Lasserre~\cite{Lasserre01moments}]
\label{th:densesdp}
Let $p_d^{\star}$ be the optimal value of the sparse SDP relaxation~$(\P_d)$.
Then, the sequence of optimal values $(p_d^\star)_{d \in \N}$ is nondecreasing and converges to $r^\star$.
\end{theorem}
%
The size of the truncated SDP variables
grows polynomially with the SDP-relaxation order $d$.
Indeed, at fixed $n$, the relaxation~$(\P_d)$ involves $O((2 d)^{n})$ SDP
variables and $(k + 1)$ linear matrix inequalities (LMIs) of size
$O(d^n)$. When $d$ increases, then more accurate lower bounds of $r^\star$ can be obtained, at an increasing computational cost.
At fixed $d$,  the relaxation $(\P_d)$ involves $O(n^{2d})$ SDP variables and $(d + 1)$ linear matrix inequalities (LMIs) of size
$O(n^{d})$.

\if{
There are several ways to decrease the size of the SDP problems. 
First, symmetries in SDP relaxations for polynomial optimization problems can be exploited to replace one SDP problem~$(\P_d)$ by
several smaller SDPs~\cite{Riener2013SymmetricSDP}. Notice that it is possible only if the multivariate polynomials of the initial problem are invariant under the action of a finite subgroup $G$ of the group $GL_{n}(\R)$. 
}\fi
%
\paragraph{Exploiting sparsity}
%
Here we remind how to exploit the structured sparsity of the
problem to replace one SDP problem~$(\P_d)$ by an SDP problem~$(\S_d)$ of
size $O (\kappa^ {2 d})$ where $\kappa$ is the average size
of the maximal cliques of the correlation pattern of the polynomial
variables (see~\cite{Waki06SparseSOS,Las06SparseSOS} for more details). We now present these notions as well as the formulation of sparse SDP relaxations~$(\S_d)$.

We note $\N^n$ the set of $n$-tuple of nonnegative integers. The support of a polynomial $r(\x) := \sum_{\alphab \in \N^n} r_{\alphab} \x^{\alphab}$ is defined as $\suppf{r} := \{ \, \alphab \in \N^n \, : \, r_{\alphab} \neq 0 \, \}$. For instance the support of $p(\x) := \frac{1}{4} + x_1^4 - 2 x_1^2 x_2^2 + x_2^4$ is $\suppf{p} = \{ \, (0,0), (4, 0), (2,2), (0,4) \, \}$.

Let $F_j$ be the index set of variables which are involved in the polynomial $g_j$, for each $j=1, \dots, k$.
The correlative sparsity is represented by the 
$n \times n$ correlative sparsity matrix (csp matrix) $\mathbf{R}$ defined by:
\begin{equation*}
\label{eq:csp}
\mathbf{R}(i, j) := \left \{
\begin{array}{ll}
  1 & \text{ if }  i = j \enspace, \\
  1 & \text{ if }  \exists \alphab \in \suppf{f} \text{ such that } \alpha_i, \alpha_j \geq 1 \,, \\
  1 & \text{ if }  \exists k \in \{1, \dots, m\} \text{ such that } i, j \in F_k  \,,\\
  0 & \text{otherwise .} 
\end{array} \right.
\end{equation*}

We define the undirected csp graph $G(N, E)$ with
 $N = \{ 1, \dots, n \}$ and $E = \{\{i, j\} : i, j \in N , \ i < j , \mathbf{R}(i, j) = 1 \}$. 
Then, let $C_1,\dots, C_m \subseteq N$ denote the maximal cliques of $G(N, E)$ and 
%and define the sets of supports: 
%\[\mons{d}{C_q} := \{ \alphab \in  \mons{d}{n} : \alpha_i = 0 \text{ if } i %\notin C_q \}, \ (q=1 ,\dots,l)\enspace. \]
 define $n_j := \#C_j$, for each $j=1 ,\dots,m$.

\begin{remark}
\label{rk:sparsearch}
From the assumption of Remark~\ref{rk:arch}, one can add the $m$ redundant additional constraints:
\begin{equation}
\label{eq:assum_sos_sparse}
g_{k + j} := n_j M^2 - \sum_{i \in C_j} {x_i^2} \geq 0\,, \  j=1 ,\dots, m \,,
\end{equation}
set $k' = k + m$, define the compact semialgebraic set:
\[\K' := \{\, \x \in \R^n \, : \, g_1 (\x) \geq 0, \dots, g_{k'} (\x) \geq 0 \,\} \,,\]
and modify Problem~\eqref{eq:minpop} into the following optimization problem:
\begin{equation}
\label{eq:sparseminpop}
r^*  :=  \inf_{\x \in \R^n} \, \{ \, r (\x) \, : \, \x \in \K' \, \} \,.
\end{equation}
\end{remark}
%Notice that the sums of squares of polynomials that belong to $\Sigma [\x, \mons{d}{C_q}]$ only involve variables $x_i (i \in C_q)$.
For each $j=1 ,\dots,m$, we note $\R_{2 d}[\x, C_j]$ the set of polynomials of $\R_{2 d}[\x]$ which involve the variables $(x_i)_{i \in C_j}$. We note $\Sigma [\x, C_j] := \Sigma [\x] \bigcap \R_{2 d}[\x, C_j]$. Similarly, we define $\Sigma [\x, F_j]$, for each $j=1, \dots, k$.
%
The following program is the sparse variant of the SDP program $(\P_d)$:
\[
(\S_d):\left\{			
\begin{array}{rl}
r_d^\star := \sup\limits_{\mu, \sigma_j} & \mu\enspace, \\	 
\text{s.t.} & r (\x) - \mu = \sum_{j = 0}^{k'} \sigma_j(\x) g_j(\x) \,, \ \forall \x \,, \\
\\
& \mu\in \R \,,\  \sigma_0 \in \sum_{j = 1}^m \Sigma [\x, C_j] \,, \\
\\
& \sigma_j \in \Sigma[\x, F_j]  \,,\ j = 1,\dots,k' \,, \\
\\
& \deg (\sigma_j g_j) \leq 2 d  \,,\ j = 0,\dots,k' \,,
\end{array} \right.
\]
%
where $\sigma \in \sum_{j = 1}^m \Sigma [\x, C_j]$ if and only if there exist $\sigma^1 \in \Sigma[\x, C_1], \dots, \sigma^m \in \Sigma[\x,C_m]$ such that $\sigma (\x) = \sum_{j = 1}^m \sigma^j (\x)$, for all $\x \in \R^n$.
%

The number of SDP variables of the relaxation~$(\S_d)$ is $\sum_{j=1}^m \binom{n_j + 2 d}{2 d}$. At fixed $d$, it yields an SDP problem with $O(\kappa^{2d})$ variables, where $\kappa := \frac{1}{m} \sum_{j=1}^m n_j$ is the average size of the cliques $C_1, \dots, C_m$.
%
Moreover, the cliques $C_1, \dots, C_m$ satisfy the running intersection property: 
%
\begin{definition}[RIP]
\label{def:rip}
Let $m \in \N_0$  and $I_1, \dots, I_m$ be subsets of $\{1, \dots, n\}$. We say that $I_1, \dots, I_m$ satisfy the running intersection property (RIP) when for all $i=1, \dots, m$, there exists an integer $l < i$ such that $I_i \cap (\cup_{j < i} I_j) \subseteq I_l$.
%\[ \forall i = 2, \dots, r, (\exists k < i \,, \ (I_i \bigcap \bigcup_{j < i} I_j) \subset I_k)\enspace. \]
\end{definition}
This RIP property together with the assumption mentioned in Remark~\ref{rk:sparsearch} allow to state the sparse variant of Theorem~\ref{th:densesdp}:
%The optimal values of $\r_d$ converge to the global minimum $f^*$, as a corollary of~\cite[Theorem 3.6]{Las06SparseSOS}.
%
\begin{theorem}[\protect{Lasserre~\cite[Theorem 3.6]{Las06SparseSOS}}]
\label{th:sparsesdp}
Let $r_d^{\star}$ be the optimal value of the sparse SDP relaxation~$(\S_d)$. Then the sequence $(r_d^{\star})_{d \in \N}$ is nondecreasing and converges to $r^\star$.
\end{theorem}
The interested reader can find more details in~\cite{Waki06SparseSOS} about additional ways to exploit sparsity in order to derive analogous sparse SDP relaxations.
We illustrate the benefits of the SDP relaxations~$(\S_d)$ with the following example:
\begin{example}
\label{ex:sparse}
Consider the polynomial $f$ mentioned in Section~\ref{sec:intro}:
$f(\x) := x_2 x_5 + x_3 x_6 - x_2 x_3  - x_5 x_6 
+ x_1 ( - x_1 +  x_2 +  x_3  - x_4 +  x_5 +  x_6)$.
%
Here, $n = 6, d = 2, N = \{1,\dots, 6 \}$. The $6 \times 6$ correlative sparsity matrix $\mathbf{R}$ is:
\[
\mathbf{R} = 
\begin{pmatrix}
  1 & 1 & 1 & 1 & 1 & 1 \\
  1 & 1 & 1 & 0 & 1 & 0 \\
  1 & 1 & 1 & 0 & 1 & 1 \\
  1 & 0 & 0 & 1 & 0 & 0 \\
  1 & 1 & 1 & 0 & 1 & 1 \\
  1 & 0 & 1 & 0 & 1 & 1 
 \end{pmatrix}
\]
The csp graph $G$ associated to $\mathbf{R}$ is depicted in Figure~\ref{fig:csp_deltax}. 
%
\begin{figure}[!ht]	
\begin{center}
\includegraphics[scale=0.6]{csp_deltax.pdf}
\caption{Correlative sparsity pattern graph for the variables of $f$}
\label{fig:csp_deltax}
\end{center}
\end{figure}
%
The maximal cliques of $G$ are $C_1 := \{1, 4\}$, $C_2 := \{1, 2, 3, 5\}$ and $C_3 := \{1, 3, 5, 6\}$. For $d=2$, the dense SDP relaxation~$(\P_2)$ involves $\binom{6 + 4}{4} = 210$ variables against $\binom{2 + 4}{4} + 2 \binom{4 + 4}{4} = 115$ for the sparse variant~$(\S_2)$. The dense SDP relaxation~$(\P_3)$ involves $924$ variables against $448$ for the sparse variant~$(\S_3)$. 
This difference becomes significant while considering that the time complexity of semidefinite programming is polynomial w.r.t. the number of variables with an exponent greater than 3 (see~\cite[Chapter 4]{BenTal01} for more details).
\end{example}
%
\subsection{Computer proofs for polynomial optimization}
\label{sec:coqbackground}
Here, we briefly recall some existing features of the $\coq$ proof assistant to handle formal polynomial optimization, when using SDP relaxations.
The advantage of such relaxations is that they provide SOS certificates, which can be formally checked \textit{a-posteriori}.
%about the mechanisms of formal polynomial optimization within the $\coq$ proof assistant.
For more details on $\coq$, we recommend the
documentation available in~\cite{bertot2004interactive}.
Giving a polynomial $r$ and a set of constraints $\K$, one can obtain a lower bound of $r$ by solving any instance of Problem~$(\P_d)$. Then, one can verify formally the correctness of the lower bound $r_d^\star$, using the SOS certificate output $\sigma_0, \dots, \sigma_k$. Indeed it is enough to prove the polynomial equality $r(\x) - r_d^\star = \sum_{j=0}^k \sigma_j(\x) g_j(\x)$ inside $\coq$. Such equalities can be efficiently proved using $\coq$'s ring tactic~\cite{ring05} via the mechanism of computational reflection~\cite{Boutin97usingreflection}. Any polynomial of type \code{pexprC} (see Section~\ref{sec:fpbackground}) can be normalized to a unique polynomial of type \code{polC} (see~\cite{ring05} for more details on the constructors of this type).
For the sake of clarity, let consider the unconstrained case, i.e.~$\K = \R^n$. One encodes an SOS certificate $\sigma_0(\x) = \sum_{i=1}^m q_i^2$  with the sequence of polynomials $[q_1; \dots; q_m]$, with each $q_i$ being of type \code{polC} . To prove the equality $r = \sigma_0$, our version of the ring tactic normalizes both $r$ and the sequence $[q_1; \dots; q_m]$ and compares the two normalization results. This mechanism is illustrated in Figure~\ref{fig:reflexion} with the polynomial $p := \frac{1}{4} + x_1^4 - 2 x_1^2 x_2^2 + x_2^4$ (see Example~\ref{ex:sdp}) being encoded by \code{p}  and the polynomials $1/2$ and $x_1^2 - x_2^2$ being encoded respectively by $\mathtt{q_1}$ and $\mathtt{q_2}$. 

\if{
In the general case, one applies a correctness lemma:
\begin{lstlisting}
Lemma correct_pop env $r$ cert_pop $ $:  
g_nonneg env g $\to$ checker g $\fpop$ $\mu_k^-$ cert_pop  $\eqcoq$ true $\to$ 
$\mu_k^-$ $\leq$ $\evalexpr{\fpop}$.
\end{lstlisting}
}\fi
%
\begin{figure}[!ht]
\centering
\includegraphics[scale=0.75]{reflexion.pdf}
\caption{An illustration of computational reflection}	
\label{fig:reflexion}
\end{figure}
%
In the general case, this computational step is done through a \code{checker_sos} procedure which returns a Boolean value. If this value is true, one applies a correctness lemma, whose conclusion yields the nonnegativity of $r - r_d^\star$ over $\K$.
In practice, the SDP solvers are implemented in floating-point precision, thus the above equality between $r - r_d^\star$ and the SOS certificate does not hold. However, following Remark~\ref{rk:arch}, each variable lies in a closed interval, thus one can bound the remainder polynomial $\epsilon(\x) := r(\x) - r_d^\star - \sum_{j=0}^k \sigma_j(\x) g_j(\x)$ using basic interval arithmetic, so that the lower bound $\epsilon^\star$ of $\epsilon$ yields the valid inequality: $\forall \x \in \K, r(\x) \geq r_d^\star + \epsilon^\star$.
For more explanation, we refer the interested reader to the formal framework~\cite{jfr14}. Note that this formal verification remains valid when considering the sparse variant~$(\S_d)$.
%
%\vspace*{-0.2cm}
\section{Guaranteed Roundoff Error Bounds using SDP Relaxations}
\label{sec:fpsdp}
In this section, we present our new algorithm to bound roundoff errors of nonlinear transcendental programs, relying on sparse SDP relaxations. After stating our general algorithm (Section~\ref{sec:transcsdp}), we detail how this procedure can handle polynomial programs (Section~\ref{sec:polsdp}) and then present extensions to the non-polynomial case (Section~\ref{sec:nonpolsdp}).
%\vspace*{-0.2cm}
\subsection{The General Optimization Framework}
\label{sec:transcsdp}
%
Here we consider a given program which implements a nonlinear transcendental expression $f$ with input variables $\x$ satisfying a set of constraints $\X$. We assume that  $\X$ is included in a box (i.e.~a product of closed intervals) $[\a, \b] := [a_1, b_1] \times \dots \times [a_n, b_n]$ and that $\X$ is encoded as follows: 
\[ 
\X := \{\, \x \in \R^n \, : \, g_1 (\x) \geq 0, \dots, g_{k} (\x) \geq 0 \,\} \,,
\]
for polynomial functions $g_1, \dots, g_k$. 
%
Then, we denote by $\hat{f}(\x,\e)$ the rounded expression of $f$ after applying the ~\lstinline|round| procedure (see Section~\ref{sec:fpbackground}), introducing additional error variables $\e$. 

The algorithm \code{bound}, depicted in Figure~\ref{alg:bound}, takes as input $\x$, $\X$, $f$, $\hat{f}$, $\e$ as well as the set $\E$ of bound constraints over $\e$. Here we assume that our program implementing $f$ does not involve conditional statements (this case will be discussed later in Section~\ref{sec:nonpolsdp}). For a given machine $\epsilon$, one has $\E := [-\epsilon, \epsilon]^m$, with $m$ being the number of error variables. This algorithm actually relies on the sparse SDP optimization procedure $(\S_d)$ (see Section~\ref{sec:sdpbackground} for more details), thus~\code{bound} also takes as input a relaxation order $d \in \N$. The algorithm provides as output an interval enclosure $I_d$ of the absolute error $\mid \hat{f}(\x,\e) - f(\x) \mid$ over $\K$. 
From this interval $I_d:= [\underline{f_d}, \overline{f_d}]$, one can compute $f_d := \max \{- \underline{f_d}, \overline{f_d} \}$, which is a sound upper bound of the maximal absolute error $r^\star := \max_{(\x,\e)\in \K} \mid \hat{f}(\x,\e) - f(\x) \mid $.

\begin{figure}[!ht]
\begin{algorithmic}[1]                    
\Require input variables $\x$, input constraints $\X$, nonlinear expression $f$, rounded expression $\hat{f}$, error variables $\e$, error constraints $\E$, relaxation order $d$
\Ensure interval enclosure $I_d$ of the absolute error $\mid \hat{f} - f  \mid$ over $\K := \X \times \E$
\State Define the absolute error $r(\x, \e) := \hat{f}(\x,\e) - f(\x)$ \label{line:r}
\State Compute $l(\x,\e) := r(\x, 0) + \sum_{j=1}^m \frac{\partial r(\x,\e)} {\partial e_j} (\x,0) \, e_j$ \label{line:l}
\State Compute $h := r - l$ \label{line:h}
\State Compute interval bounds for $h$: $I^h := \iaboundfun{h}{\K}$ \label{line:iabound}
\State Compute interval bounds for $l$: $I_d^l := \sdpboundfun{l}{\K}{d}$  \label{line:sdpbound}
\State \Return $I_d := I_d^l + I^h$ 
\end{algorithmic}
\caption{\code{bound}: our algorithm to compute roundoff errors bounds of nonlinear programs}
\label{alg:bound}
\end{figure}

After computing the absolute roundoff error $r := \hat{f} - f$ (Line~\lineref{line:r}), one decomposes $r$ as the sum of an expression $l$ which is affine w.r.t.~the error variable $\e$ and a remainder $h$. One way to obtain $l$ is to compute the vector of partial derivatives of $r$ w.r.t.~$\e$ evaluated at $(\x, 0)$ and finally to take the inner product of this vector and $\e$ (Line~\lineref{line:l}). Then, the idea is to compute a precise bound of $l$ and a coarse bound of $h$. The underlying reason is that $h$ involves error term products of degree greater than 2 (e.g.~$e_1 e_2$), yielding an interval enclosure $I^h$ of \textit{a priori} much smaller width, compared to the interval enclosure $I^l$ of $l$. One obtains $I^h$ using the procedure $\iabound$ implementing basic interval arithmetic (Line~\lineref{line:iabound}). 
%
%\vspace*{-0.2cm}
\subsection{Polynomial Programs}
\label{sec:polsdp}
%
We first describe our $\sdpbound$ optimization algorithm when implementing polynomial programs. In this case, $\sdpbound$ calls an auxiliary procedure $\sdppoly$.
The bound of $l$ is provided through solving two sparse SDP instances of Problem~$(\S_d)$, at relaxation order $d$. We now give more explanation about the $\sdppoly$ procedure.

We can map each input variable $x_i$ to the integer $i$, for all $i=1,\dots,n$, as well as each error variable $e_j$ to $n+j$, for all $j=1,\dots,m$. Then, define the sets $C_1 := \{1,\dots,n,n+1\}, \dots, C_m := \{1,\dots,n,n+m\}$. Here, we take advantage of the sparsity correlation pattern of $l$ by using $m$ distinct sets of cardinality $n+1$ rather than a single one of cardinality $n+m$, i.e.~the total number of variables. 
After noticing that $r(\x,0) = \hat{f}(\x,0) - f(\x) = 0$, one can scale the optimization problems by writing 
\begin{align}
\label{eq:lscale}
l(\x,\e) = \sum_{j=1}^m s_j (\x) e_j = \epsilon \sum_{j=1}^m s_j (\x) \frac{e_j}{\epsilon} \,,
\end{align}
%
with $s_j(\x) := \frac{\partial r(\x,\e)} {\partial e_j} (\x,0)$, for all $j=1,\dots,m$. Replacing $\e$ by $\e/\epsilon$ leads to compute an interval enclosure of $l/\epsilon$ over $\K' := \X \times [-1, 1]^m$.
Recall that from Remark~\ref{rk:arch}, there exists an integer $M > 0$ such that $M - \sum_{i=1}^n x_i^2 \geq 0$, as the input variable satisfy box constraints.
Moreover, to fulfil the assumption of Remark~\ref{rk:sparsearch},  one encodes $\K'$ as follows: 
\begin{align*}
\K' := \{\, (\x,\e) \in \R^{n+m} \, : \, g_1 (\x) \geq 0, \dots, g_k(\x) \geq 0 \,, \\
g_{k+1}(\x,e_1) \geq 0, \dots, g_{k+m} (\x, e_m) \geq 0 \,\} \,,
\end{align*}
%
with $g_{k+j}(\x, e_j) := M + 1 -  \sum_{i=1}^n x_i^2 - e_j^2$, for all $j=1,\dots, m$. 
The index set of variables involved in $g_j$ is $F_j := N = \{1, \dots, n\}$ for all $j=1, \dots, k$. 
The index set of variables involved in $g_{k+j}$ is $F_j := C_j$ for all $j=1, \dots, m$. 

Then, one can compute a lower bound of the minimum of $l'(\x,\e) := l(\x, \e) / \epsilon$ over $\K'$ by solving the following optimization problem:
%
\begin{align}
\begin{split}
\label{eq:lscalesdp1}
\left\{			
\begin{array}{rl}
\underline{l_d'} := \sup\limits_{\mu, \sigma_j} & \mu\enspace, \\	 
\text{s.t.} & l' - \mu = \sigma_0 + \sum_{j = 1}^{k+m} \sigma_j g_j \,, \\
\\
& \mu\in \R \,,\ \sigma_0 \in \sum_{j = 1}^m \Sigma [(\x, \e), C_j] \,, \\
\\
& \sigma_j \in \Sigma[(\x,\e), F_j] \,, \ j = 1,\dots,k+m \,, \\
\\
& \deg (\sigma_j g_j) \leq 2 d  \,, \ j = 1,\dots,k+m \,.
\end{array} \right.
\end{split}
\end{align}
%
A feasible solution of Problem~\eqref{eq:lscalesdp1} ensures the existence of $\sigma^1 \in \Sigma[(\x,e_1)], \dots, \sigma^m \in \Sigma[(\x,e_m)]$ such that $\sigma_0 = \sum_{j=0}^m \sigma^j$, allowing the following reformulation:
%
\begin{align}
\begin{split}
\label{eq:lscalesdp2}
\left\{			
\begin{array}{rl}
\underline{l_d'} := \sup\limits_{\mu, \sigma_j} & \mu\enspace, \\	
\text{s.t.} & l' - \mu = \sum_{j=1}^m \sigma^j + \sum_{j = 1}^{k+m} \sigma_j g_j \,, \\
\\
& \mu\in \R \,, \ \sigma_j \in \Sigma[\x] \,, \ j = 1,\dots,m \,, \\
\\
& \sigma^j  \in \Sigma [(\x, e_j)] \,,  \deg (\sigma^j) \leq 2 d  \,, \ j = 1,\dots,m \,, \\
\\
&  \quad \deg (\sigma_j g_j) \leq 2 d  \,, \ j = 1,\dots,k+m \,.
\end{array} \right.
\end{split}
\end{align}
%
An upper bound $\overline{l_d'}$ can be obtained by replacing $\sup$ with $\inf$ and $l' - \mu$ by $\mu - l'$ in Problem~\eqref{eq:lscalesdp2}.
Our optimization procedure $\sdppoly$ computes the lower bound $\underline{l_d'}$ as well as an upper bound $\overline{l_d'}$ of $l'$ over $\K'$ then returns the interval $I_d^l := [\epsilon \, \underline{l_d'}, \epsilon \, \overline{l_d'}] $, which is a sound enclosure of the values of $l$ over $\K$.
%
%\begin{remark}

We emphasize two advantages of the decomposition $r := l + h$ and more precisely of the linear dependency of $l$ w.r.t.~$\e$: scalability and robustness to SDP numerical issues.
First, no computation is required to determine the correlation sparsity pattern of $l$, by comparison to the general case. Thus, it becomes much easier to handle the optimization of $l$ with the sparse SDP Problem~\eqref{eq:lscalesdp2} rather than with the corresponding instance of the dense relaxation~$(\P_d)$. While the latter involves $\binom{n + m+ 2 d}{2 d}$ SDP variables, the former involves only $m \, \binom{n + 1 + 2 d}{2 d}$ variables, ensuring the scalability of our framework.
%
In addition, the linear dependency of $l$ w.r.t.~$\e$ allows us to scale the error variables and optimize over a set of variables lying in $\K' := \X \times [-1, 1]$. It ensures that the range of input variables does not significantly differ from the range of error variables. This condition is mandatory while considering SDP relaxations because most SDP solvers (e.g.~{\sc Mosek}~\cite{mosek}, {\sc SDPA}~\cite{sdpa7}) are implemented using double precision floating-point. It is impossible to optimize $l$ over $\K$ (rather than $l'$ over $\K'$) when the maximal value $\epsilon$ of error variables is less than $2^{-53}$, due to the fact that SDP solvers would treat each error variable term as 0, and consequently $l$ as the zero polynomial. Hence, this decomposition insures our framework from the numerical issues related to finite-precision implementation of SDP solvers.
%\end{remark}

%
Let us define the interval enclosure $I^l := [\underline{l}, \overline{l}]$, with $\underline{l} := \inf_{(\x,\e) \in \K} l(\x,\e)$ and $\overline{l} := \sup_{(\x,\e) \in \K} l(\x,\e)$.
The next lemma states that one can approximate $I^l$ as closely as desired using the $\sdppoly$ procedure.
\begin{lemma}[Convergence of the $\sdppoly$ procedure]
\label{th:cvg_sdppoly}
Let $I_d^l$ be the interval enclosure returned by the procedure $\sdppolyfun{l}{\K}{d}$. The sequence $(I_d^l)_{d \in \N}$ converges to $I^l$.
\end{lemma}
%
%\vspace*{-0.2cm}
\begin{proof}
It is sufficient to show the similar convergence result for $l' = l/\epsilon$, as it implies the convergence for $l$ by a scaling argument.
The sets $C_1,\dots, C_m$ satisfy the RIP property (see Definition~\ref{def:rip}). Moreover, the encoding of $\K'$ satisfies the assumption mentioned in Remark~\ref{rk:sparsearch}. Thus, Theorem~\ref{th:sparsesdp} implies that the sequence of lower bounds $(\underline{l_d'})_{d \in \N}$ converges to $\underline{l'} := \inf_{(\x,\e) \in \K'} l'(\x,\e)$. Similarly, the sequence of upper bounds converge to $\overline{l'}$, yielding the desired result.
\end{proof}
%
Lemma~\ref{th:cvg_sdppoly} guarantees asymptotic convergence to the exact enclosure of $l$ when the relaxation order $d$ tends to infinity. However, it is more reasonable in practice to keep this order as small as possible to obtain tractable SDP relaxations. Hence, we generically solve each instance of Problem~\eqref{eq:lscalesdp2} at the minimal relaxation order, that is $d_0 := \max \{\lceil \deg l / 2\rceil) , \max_{1 \leq j \leq k+m} \{ \lceil \deg (g_j) / 2\rceil) \} \}$. 
%
\subsection{Non-polynomial and Conditional Programs}
\label{sec:nonpolsdp}
Other classes of programs do not only involve polynomials but also semialgebraic and transcendental functions as well as conditional statements. Such programs are of particular interest as they often occur in real-world applications such as biology modeling, space control or global optimization. We present how the general optimization procedure $\sdpbound$ can be extended to these nonlinear programs.
%, including semialgebraic and transcendental functions. Then, we treat the case of programs involving conditional statements. We emphasize the interest of these programs, as they occur numerously in real-world applications such as biology modeling or space control. 
%
%\vspace*{-0.1cm}
\paragraph{Semialgebraic programs}
Here we assume that the function $l$ is semialgebraic, that is involves non-polynomial components such as divisions or square roots.
Following~\cite{LasPut10}, we explain how to transform the optimization problem $\inf_{(\x,\e) \in \K} l (\x, \e)$ into a polynomial optimization problem, then use the sparse SDP program~\eqref{eq:lscalesdp2}. One way to perform this reformulation consists of introducing lifting variables to represent non-polynomial operations.
%
We first illustrate the extension to semialgebraic programs with an example.
\begin{example}
Let consider the program implementing the rational function $f : [0, 1] \to \R$ defined by $f(x_1) := \frac{x_1}{1 + x_1}$. Applying the rounding procedure (with machine $\epsilon$) yields $\hat{f}(x_1,\e) := \frac{x _1(1 + e_2)}{(1 + x_1)(1 + e_1)}$ and the decomposition $r(x_1, \e) := \hat{f}(x_1,\e) - f(x_1) = l(x_1,\e) + h(x_1\,e) = s_1 (x_1) e_1 + s_2 (x_1) e_2 + h(x_1,\e)$. One has $s_1(x_1) = \frac{\partial r(x_1,\e)} {\partial e_1} (x_1,0) = -\frac{x_1}{1 + x_1}$ and $s_2(x_1) = - s_1(x_1)$.

Let $\K := [0, 1] \times [-\epsilon, \epsilon]^2$. One introduces a lifting variable $x_2 := \frac{x_1}{1 + x_1}$ to handle the division operator and encode the equality constraint $p(\x) :=  x_2 (1 + x_1) - x_1 = 0$ with the two inequality constraints $p (\x) \geq 0$ and $-p(\x) \geq 0$. To ensure the compactness assumption, one bounds $x_2$ within $I := [0, 1/2]$, using basic interval arithmetic.

Let $\Kpol := \{(\x,\e) \in [0, 1] \times I \times [\epsilon, \epsilon]^2 : p(\x) \geq 0 \,,\  - p(\x) \geq 0 \}$. Then the rational optimization problem involving $l$ is equivalent to $\inf_{(\x,\e) \in \Kpol} x_2 (-e_1 + e_2)$, a polynomial optimization problem that we can handle with the $\sdppoly$ procedure, described in Section~\ref{sec:polsdp}.
\end{example}
%

In the semialgebraic case, $\sdpbound$ calls an auxiliary procedure $\sdpsa$. 
Given input variables $\y := (\x,\e)$, input constraints $\K := \X \times \E$ and a 
semialgebraic function $l$,  $\sdpsa$ first applies a recursive procedure $\lift$ 
which returns variables $\y\poly$, constraints $\K\poly$ and a polynomial $f\poly$ such that the 
interval enclosure $I^l$ of $l(\y)$ over $\K$ is equal to the interval enclosure of 
the polynomial $l\poly(\y\poly)$ over $\K\poly$. 
Calling $\sdpsa$ yields the interval enclosure $I^l_d := \sdppolyfun{l\poly}{\K\poly}{d}$. We detail the lifting procedure $\lift$ in Figure~\ref{alg:lift} for the constructors \code{Pol}(Line~\eqref{line:liftpol}), \code{Div} (Line~\eqref{line:liftdiv}) and \code{Sqrt} (Line~\eqref{line:liftsqrt}). 
The interval $I$ obtained through the $\iabound$ procedure (Line~\eqref{line:liftia}) allows us to constrain the additional variable $x$ to ensure the assumption of Remark~\ref{rk:sparsearch}.
For the sake of consistency, we omit the other cases (\code{Neg}, \code{Add}, \code{Mul} and \code{Sub}) where the procedure is straightforward. For a similar procedure in the context of global optimization, we refer the interested reader to~\cite[Chapter 2]{MagronPhD}.

\begin{figure}[!ht]
\begin{algorithmic}[1]                 
\Require input variables $\y$, input constraints $\K$, semialgebraic expression $f$
\Ensure variables $\y\poly$, constraints $\K\poly$, polynomial expression $f\poly$
    \State $I := \iaboundfun{f}{\K}$ \label{line:liftia}
	\If {\lstinline|$f = \ $ Pol ($p$)|} \label{line:liftpol} $\y\poly := \y$, $\K\poly := \K$, $\f\poly := p$
	\ElsIf {\lstinline|$f = \ $ Div ($g$, $h$)|} \label{line:liftdiv} 
	\State $\y_g, \K_g, g\poly := \liftfun{\y}{\K}{g}$ 
	\State $\y_h, \K_h, h\poly := \liftfun{\y}{\K}{h}$
    \State $\y\poly := (\y_g,\y_h,x)$ \hspace{1cm} $f\poly := x$ 
	\State $\K\poly := \{\y\poly \in \K_g \times \K_h \times I : x g\poly = f\poly \}$
	\ElsIf {\lstinline|$f = \ $ Sqrt ($g$)|} \label{line:liftsqrt} 
	\State $\y_g, \K_g, g\poly := \liftfun{\y}{\K}{g}$ 
	\State $\y\poly := (\y_g,x)$ \hspace{1cm} $f\poly := x$ 
	\State $\K\poly := \{\y\poly \in \K_g \times I : x^2 = g\poly \}$\\
\code{...}
	\EndIf
%
\State \Return $\y\poly, \K\poly, f\poly$
\end{algorithmic}
\caption{\code{lift}: a recursive procedure to reduce semialgebraic problems to polynomial problems}
\label{alg:lift}
\end{figure}

The set of variables $\y\poly$ can be decomposed as $(\x\poly, \e)$, where $\x\poly$ gathers input variables with lifting variables and has a cardinality equal to $n\poly$.
Then, one easily shows that the sets $\{1, \dots, n\poly, e_1\}$,$\dots$,$\{1, \dots, n\poly, e_m\}$ satisfy the RIP, thus ensuring to solve efficiently the corresponding instances of Problem~\eqref{eq:lscalesdp2}.
%
\paragraph{Transcendental programs}
%
Here we assume that the function $l$ is transcendental,~i.e.~involves univariate non-semialgebraic components such as $\exp$ or $\sin$. For each univariate transcendental function $f_{\R}$ in our dictionary set $\setD$, one assumes that $f_{\R}$ is twice differentiable, so that the univariate function $g := f_{\R} + \frac{\gamma}{2} |\cdot|^2$ is convex on $I$ for large enough $\gamma > 0$ (for more details, see the references~\cite{agk04, mceneaney-livre}). It follows that there exists a constant $\gamma \leq \sup_{x\in I} -f_{\R}''(x)$ such that for all $x_i \in I$:
\begin{align}
\begin{split}
\label{eq:maxplus}
\forall x \in I, \quad f_{\R} (x)  \geq f_{x_i}^-(x) \,,\\
\text{with } f_{x_i}^- :=  -\frac{\gamma}{2} (x-x_i)^2 +f_{\R}'(x_i) (x - x_i) + f_{\R} (x_i) \,,
\end{split}
\end{align}
implying that for all $x \in I$, $f_{\R} (x)  \geq \max_{x_i \in I} f_{x_i}^-(x)$. Similarly, one obtains an upper-approximation $\min_{x_i \in I} f_{x_i}^+(x)$.
Figure~\ref{fig:logexp} provides such approximations for the function $f_{\R}(x) := \log (1 + \exp(x))$ on the interval $I := [-8, 8]$.
%
\begin{figure}[!ht]
\begin{center}
\includegraphics[scale=0.7]{logexp.pdf}
\caption{Maxplus Approximations for $x \mapsto \log(1 + \exp{x})$: $\max \{ f_0^- (x), f_8^- (x)\} \leq  \log(1 + \exp{x}) \leq \min \{ f_0^+ (x), f_8^+ (x)\} $}\label{fig:logexp}
\end{center}
\end{figure}
%

For transcendental programs, our procedure $\sdpbound$ calls the auxiliary procedure $\sdptransc$. Given input variables $(\x,\e)$, constraints $\K$ and a transcendental function $l$, $\sdptransc$ first computes a semialgebraic lower (resp.~upper)  approximation $l^-$ (resp.~$l^+$) of $l$ over $\K$. For more details in the context of global optimization, we refer the reader to~\cite{Magron15sdp}. Then, calling the procedure $\sdpsa$ allows us to get interval enclosures of $l^-$ as well as $l^+$.
We illustrate the procedure to handle transcendental programs with an example.
%
\begin{example}
\label{ex:logexp}
Let consider the program implementing the transcendental function $f : [-8, 8] \to \R$ defined by $f(x_1) := \log (1 + \exp(x_1))$. Applying the rounding procedure  yields $\hat{f}(x_1,\e) := \log [(1 + \exp(x_1) (1 + e_1)) \, (1 + e_2)](1 + e_3)$. 
Here, $|e_2|$ is bounded by the machine $\epsilon$ while $|e_1|$ (resp.~$|e_3|$) is bounded with an adjusted absolute error $\epsilon_1 := \epsilon(\exp)$ (resp.~$\epsilon_3 := \epsilon(\log)$).
Let $\K:= [-8,8] \times [-\epsilon_1, \epsilon_1] \times [\epsilon, \epsilon] \times [-\epsilon_3, \epsilon_3]$.

One obtains the decomposition $r(x_1, \e) := \hat{f}(x_1,\e) - f(x_1) = l(x_1,\e) + h(x_1,\e) = s_1 (x_1) e_1 + s_2 (x_1) e_2 + s_3 (x_1) e_3 + h(x_1, \e)$, with 
$s_1(x_1) = \frac{\exp(x_1)} {1 + \exp(x_1)}$, $s_2(x_1) = 1$ and $s_3(x_1) = \log (1 + \exp(x_1)) = f(x_1)$. Figure~\ref{fig:logexp} provides a lower approximation $s_3^- := \max\{f_0^-,f_8^-\}$ of $s_3$ as well as an upper approximation $s_3^+ := \min \{f_0^+,f_8^+\}$. One can get similar approximations $s_1^-$ and $s_1^+$ for $s_1$. 
%
One first obtains (coarse) interval enclosures $I_2 = \iaboundfun{s_1}{\K}$ and $I_3 = \iaboundfun{s_3}{\K}$ and one introduces extra variables $x_2 \in I_2$ and $x_3 \in I_3$ to represent $s_1$ and $s_3$ respectively.
Then, the interval enclosure of $l$ over $\K$ is equal to the interval enclosure of $l\sa(\x,\e) := x_2 e_1 + e_2 + x_3 e_3$ over the set $\K\sa:= \{(x_1,\e) \in \K \,, (x_2, x_3) \in I_2 \times I_3 \,, s_1^-(x_1) \leq x_2 \leq s_1^+(x_1) \,, s_3^-(x_1) \leq x_3 \leq s_3^+(x_1) \}$.
\end{example}
%
\paragraph{Programs with conditionals}
%
Finally, we explain how to extend our bounding procedure to nonlinear programs involving conditionals through the recursive algorithm given in Figure~\ref{alg:bound_nlprog}.
The $\boundnlprog$ algorithm relies on the $\bound$ procedure (see Figure~\ref{alg:bound} in Section~\ref{sec:transcsdp}) to compute roundoff error bounds of programs implementing transcendental functions (Line~\lineref{line:noncnd}).
From Line~\lineref{line:cnd} to Line~\lineref{line:endcnd}, the algorithm handles the case when the program implements a function $f$ defined as follows:
\[   
f (\x) := 
     \begin{cases}
       g(\x) &\text{if } p(\x) \geq 0,\\
       h(\x) &\text{otherwise}.
     \end{cases}
\]
The first branch output is $g$ while the second one is $h$.
%
%\vspace*{-0.05cm}
\begin{figure}[!ht]
\begin{algorithmic}[1]
\Require input variables $\x$, input constraints $\X$, nonlinear expression $f$, rounded expression $\hat{f}$, error variables $\e$, error constraints $\E$, relaxation order $d$
\Ensure interval enclosure $I_d$ of the error $\mid \hat{f} - f  \mid$ over $\K := \X \times \E$
%
\If{\lstinline|$f = \ $ IfThenElse ($p, g, h$)|} \label{line:cnd}
\State $I_d^p := \boundfun{\x}{\X}{p}{\hat{p}}{\e}{\E}{d} = [\underline{p_d}, \overline{p_d}]$ \label{line:polcnd}
\State $\X_1 := \{ \x \in \X : 0 \leq p(\x) \leq \overline{p_d} \}$ \label{line:X1}
\State $\X_2 := \{ \x \in \X : \underline{p_d} \leq p(\x) \leq 0 \}$\label{line:X2}
\State $\X_3 := \{ \x \in \X : 0 \leq p(\x) \}$\label{line:X3}
\State $\X_4 := \{ \x \in \X : p(\x) \leq 0 \}$\label{line:X4}
\State $I_d^1 := \boundnlprogfun{\x}{\X_1}{g}{\hat{h}}{\e}{\E}{d}$\label{line:I1}
\State $I_d^2 := \boundnlprogfun{\x}{\X_2}{h}{\hat{g}}{\e}{\E}{d}$\label{line:I2}
\State $I_d^3 := \boundnlprogfun{\x}{\X_3}{g}{\hat{g}}{\e}{\E}{d}$\label{line:I3}
\State $I_d^4 := \boundnlprogfun{\x}{\X_4}{h}{\hat{h}}{\e}{\E}{d}$\label{line:I4}
\State \Return $I_d := I_d^1 \cup I_d^2 \cup I_d^3 \cup I_d^4$ \label{line:endcnd}
\Else ~ \Return $I_d := \boundfun{\x}{\X}{f}{\hat{f}}{\e}{\E}{d}$ \label{line:noncnd}
\EndIf
%
\end{algorithmic}
\caption{\code{bound_nlprog}: our algorithm to compute roundoff error bounds of  programs with conditional statements}
\label{alg:bound_nlprog}
\end{figure}
%

A preliminary step consists of computing the roundoff error enclosure $I_d^p := [\underline{p_d}, \overline{p_d}]$ (Line~\lineref{line:polcnd}) for the program implementing the polynomial $p$. 
Then the procedure computes bounds related to the divergence path error, that is the maximal value between the four following errors: 
\begin{itemize}[noitemsep,nolistsep]
\item (Line~\lineref{line:I1}) the error obtained while computing the rounded result $\hat{h}$ of the second branch instead of computing the exact result $g$ of the first one, occurring for the set of variables $(\x,\e)$ such that $\hat{p}(\x,\e) \leq 0 \leq p(\x)$. For scalability and numerical issues, we consider an over-approximation $\X_1$ (Line~\lineref{line:X1}) of this set, where the variables $\x$ satisfy the relaxed constraints $0 \leq p(\x) \leq \overline{p_d}$.
\item (Line~\lineref{line:I2}) the error obtained while computing the rounded result $\hat{g}$ of the first branch instead of computing the exact result $h$ of the second one, occurring for the set of variables $(\x,\e)$ such that $p(\x) \leq 0 \leq \hat{p}(\x,\e)$. We also consider an over-approximation $\X_2$ (Line~\lineref{line:X2}), where the variables $\x$ satisfy the relaxed constraints $\underline{p_d} \leq p(\x) \leq 0$.
\item (Line~\lineref{line:I3}) the roundoff error corresponding to the program implementation of $g$.
\item(Line~\lineref{line:I4}) the roundoff error corresponding to the program implementation of $h$.
\end{itemize}
%
\paragraph{Simplification of error terms}
%
In addition, our algorithm \code{bound_nlprog} integrates several features to reduce the number of error variables. First, it memorizes all sub-expressions of the nonlinear expression tree to perform common sub-expressions elimination. 
%associate  the same error variable to the same operation when the operands are identical. 
%For instance, several occurrences of $x_1 + x_2$ are replaced each time by $(x_1 + x_2) (1 + e_1)$.
We can also simplify error term products, thanks to the following lemma.
\begin{lemma}[\protect{Higham~\cite[Lemma 3.3]{higham2002accuracy}}]
\label{th:redproduct}
Let $\epsilon < \frac{1}{k}$ and $\gamma_k := \frac{k \epsilon}{1 - k \epsilon}$. Then, for all $e_1, \dots, e_k \in [-\epsilon, \epsilon]$, there exists $\theta_k$ such that ${\prod_{i=1}^k (1 + e_i) = 1 + \theta_k}$ and $\mid \theta_k \mid \leq \gamma_k$.
\end{lemma}
%
Lemma~\ref{th:redproduct} implies that for any $k$ such that $\epsilon < \frac{1}{k}$, one has $\gamma_k \leq (k + 1) \epsilon$. Our algorithm has an option to automatically derive safe over-approximations of the absolute roundoff error while introducing only one variable $e_1$ (bounded by $(k + 1) \epsilon$) instead of $k$ error variables $e_1, \dots, e_k$ (bounded by $\epsilon$). The cost of solving the corresponding optimization problem can be significantly reduced but it yields coarser error bounds.
%
%\vspace*{-0.2cm}
\section{Experimental Evaluation}
\label{sec:benchs}
%
Now, we present experimental results obtained by applying our general $\boundnlprog$ algorithm (see Section~\ref{sec:fpsdp}, Figure~\ref{alg:bound_nlprog}) to various examples coming from physics, biology, space control and optimization. 
The  $\boundnlprog$ algorithm is implemented in a tool called $\realtofloat$, built in top of the $\nlcertify$ nonlinear verification package, relying on $\ocaml$ (Version $4.02.1$), $\coq$ (Version $8.4\text{pl}5$) and interfaced with the SDP solver $\sdpa$ (Version $7.3.6$). The SDP solver output numerical SOS certificates, which are converted into rational SOS using the {\sc Zarith} $\ocaml$ library (Version $1.2$), implementing arithmetic operations over arbitrary-precision integers.
For more details about the installation and usage of $\realtofloat$, we refer to the dedicated web-page\footnote{\url{http://nl-certify.forge.ocamlcore.org/real2float.html} (\textbf{not anonymized})} and the setup instructions\footnote{see the \texttt{README.md} file in the top level directory}. 
All examples are displayed in Appendix A as the corresponding $\realtofloat$ input text files and satisfy our nonlinear program semantics (see Section~\ref{sec:fpbackground}). All results have been obtained on an Intel Core i5 CPU ($2.40\, $GHz). Execution timings have been computed by averaging over five runs.
%
%\vspace*{-0.2cm}
\subsection{Benchmark Presentation}
%For each problem presented in Table~\ref{table:error}, 
For each example, we compared the quality of the roundoff error bounds (Table~\ref{table:error}) and corresponding execution times (Table~\ref{table:cpu}) while running our tool $\realtofloat$, $\fptaylor$ (version from May $2015$)~\cite{fptaylor15} and $\rosa$ (version from  May $2014$)~\cite{Darulova14Popl}.
A given program implements a nonlinear function $f(\x)$, involving variables $\x$ lying in a set $\X$ contained in a box $[\a, \b]$.
Applying our rounding model on $f$ yields the nonlinear expression $\hat{f}(\x,\e)$, involving additional error variables $\e$ lying in a set $\E$. 
At a given semidefinite relaxation order $d$, our tool computes the upper bound $f_d$ of the absolute roundoff error $\mid f - \hat{f} \mid $ over $\K := \X \times \E$ and verifies that it is less than a requested number $\epsilon_{\realtofloat}$. As we keep the relaxation order $d$ as low as possible to ensure tractable SDP programs, it can happen that $f_d > \epsilon_{\realtofloat}$. 
 In this case, we subdivide a randomly chosen interval of the box  $[\a, \b]$ in two halves to obtain two sub-sets $\X_1$ and $\X_2$, fulfilling $\X := \X_1 \cup \X_2$, and apply the $\boundnlprog$ algorithm on both sub-sets until we succeed to certify that $\epsilon_{\realtofloat}$ is a sound upper bound of the roundoff error.

The number $\epsilon_{\realtofloat}$ is compared with the upper bounds computed by $\fptaylor$, which relies on Taylor Symbolic expansions~\cite{fptaylor15}, $\rosa$, which relies on SMT and affine arithmetic~\cite{Darulova14Popl}, as well as a third procedure, IA, relying on interval arithmetic. We designed IA to follow the same steps than $\boundnlprog$, together with the sub-procedure $\bound$, but to compute an interval enclosure of $l$ with the procedure $\iabound$ using a basic interval arithmetic procedure, instead of calling the $\sdpbound$ procedure (see Line~\lineref{line:sdpbound} of the algorithm depicted in Figure~\ref{alg:bound}). For comparison purpose, we also executed each program using random inputs, following the approach used
in the $\rosa$ paper~\cite{Darulova14Popl}.  Specifically,
we executed each program on $10^7$ random inputs satisfying the input restrictions.  The results from these random
samples provide lower bounds on the absolute error.
%
\begin{table*}[!ht]
%\small
\begin{center}
\caption{Comparison results of upper and lower bounds for absolute roundoff errors (the best results are emphasized using \textbf{bold fonts})}
\input{error}
\label{table:error}
\end{center}
\end{table*}
%
For the sake of further presentation, we associate an alphabet character (from \code{a} to \code{w}) to identify each of the $\nbenchs$ nonlinear programs:
%
%\vspace*{-0.5cm}
\begin{itemize}[noitemsep,nolistsep]
\item The first $14$ programs implement polynomial and semialgebraic functions: \code{a-e} come from physics, \code{f} and \code{h} from biology, \code{g} from control, \code{i-k} are derived from expressions involved in the proof of Kepler Conjecture (for more details, see~\cite{halesalgo} and the related formalization project~\cite{Flyspeck06}) and \code{l-n} implement polynomial approximations of the sine and square root functions. With the exception of \code{i-k}, all these programs are used to compare $\fptaylor$ and $\rosa$ in~\cite{fptaylor15}. 
\item The three polynomial programs \code{o-q} come from the global optimization literature and correspond respectively to Problem 3.3, 4.6 and 4.7 in~\cite{Floudas90}. We selected them as they typically involve nontrivial polynomial preconditions (i.e. $\X$ is not a simple box but rather a set defined with conjunction of nonlinear polynomial inequalities).
\item The four programs \code{r-u} involve transcendental functions. The two programs \code{r} and \code{s} are used in the $\fptaylor$ paper~\cite{fptaylor15} and correspond respectively to the program \code{logexp} (see Example~\ref{ex:logexp}) and the program \code{sphere} taken from NASA World Wind Java SDK~\cite{NASA}. The $2$ programs \code{t} and \code{q} respectively implement the functions coming from the optimization problems \textit{Hartman 3} and \textit{Hartman 6} in~\cite{Ali05}, involving both sums of exponential functions composed with quadratic polynomials.
\item The last two programs \code{v-w} involve conditional statements and come from the static analysis literature. They correspond to the two respective running examples of~\cite{Zonotope10} and~\cite{Marechal14}. The first program \code{v} is used in the $\rosa$ paper~\cite{Darulova14Popl} for the analysis of divergence path error.
\end{itemize}
%
The three tools $\realtofloat$, $\rosa$ and $\fptaylor$ can handle input variable uncertainties as well as multi-precision programs, but for the sake of conciseness, we only considered to compare their performance on programs implemented in single ($\epsilon = 2^{-24}$) or double ($\epsilon = 2^{-53}$) precision floating-point.
For the programs involving transcendental functions, we followed the same procedure as in $\fptaylor$ while adjusting the precision $\epsilon \,  (f_{\R}) = 1.5 \epsilon$ for each special function $f_{\R} \in \{\sin, \cos, \log, \exp \}$. Each univariate transcendental function is approximated from below (resp.~from above) using suprema (resp.~infima) of linear or quadratic polynomials (see Example~\ref{ex:logexp} for the case of program \code{logexp}). 
%
%\vspace*{-0.2cm}
\subsection{Comparison Results}
%
Comparison results for error bound computation are presented in Table~\ref{table:error}. 
%
Our $\realtofloat$ tool computes the tightest upper bounds for $18$ out of $\nbenchs$ benchmarks. For the $3$ programs \code{a-c} involving rational functions and the program \code{t} encoding Problem~\textit{Hartman 3}, $\fptaylor$ computes the tightest bounds. The symbol $\divzero$ means that the IA procedure aborts with a division by zero exception, which typically occurs when computing too coarse interval enclosures of rational function denominators.
% as our tool is currently limited of $\realtofloat$ is its ability to manipulate symbolic expressions, e.g. computing rational function derivatives or yielding reduction to the same denominator.
One current limitation of $\realtofloat$ is its ability to manipulate symbolic expressions, e.g. computing rational function derivatives or yielding reduction to the same denominator. For that reason, we omitted some programs studied in~\cite{fptaylor15} (e.g.~\code{turbine}) involving rational functions.
Performance of $\fptaylor$ are better to analyze such functions, as it handles properly symbolic operations through the interface with the \textsc{Maxima} computer algebra system~\cite{maxima}.
Accordingly with the $\fptaylor$ paper~\cite{fptaylor15}, the $\rosa$ real compiler often provides coarser bounds, except for the two conditional programs \code{v} and \code{w} and the polynomial program \code{o}.
%
\begin{figure}[!ht]
\begin{center}
\includegraphics[scale=0.76]{timebound2.pdf}
\caption{Comparisons of execution times and upper bounds of roundoff errors obtained with $\rosa$ and $\fptaylor$, relatively to $\realtofloat$}\label{fig:timebound}
\end{center}
\end{figure}
%
\begin{table}[!ht]
%\small
\begin{center}
\caption{Comparison of execution times (in seconds) for absolute roundoff error bounds (the best results are emphasized using \textbf{bold fonts})}
\input{time}
\label{table:cpu}
\end{center}
\end{table}
%
Program \code{u} can only be tackled with $\realtofloat$ as $\fptaylor$ aborted after running out of memory (meaning of the symbol OoM). A possible failure explanation is the complexity of the corresponding Problem~\textit{Hartman 6}, involving $133$ arithmetic operations and $6$ input variables.
To the best of our knowledge, $\realtofloat$ is the only academic tool which is able to handle the general class of programs involving either transcendental functions or conditional statements. The $\fptaylor$ (resp.~$\rosa$) does not currently handle conditionals (resp.~transcendental functions), as meant by the symbol $-$ in the corresponding column entries. However, an interface between both software would embed them with their respective missing feature.

These error bound comparison results together with their corresponding execution timings (given in Table~\ref{table:cpu}) are used to plot the data points shown in Figure~\ref{fig:timebound}.
%
%Figure~\ref{fig:timebound} provides a way to compare both execution times and error bounds obtained with the different tools, and gathers results from Table~\ref{table:error} and Table~\ref{table:cpu}. 
For each benchmark identified by \code{id}, let $t_{\realtofloat}$ (in 3rd column of Table~\ref{table:cpu}) refer to the execution time of $\realtofloat$ to obtain the corresponding upper bound $\epsilon_{\realtofloat}$ (in 4th column of Table~\ref{table:error}). 
Similarly, let us define the execution times $t_{\rosa}$, $t_{\fptaylor}$ and the corresponding error bounds $\epsilon_{\rosa}$, $\epsilon_{\fptaylor}$. Then the x-axis coordinate of the point \circled{id} (resp.~\squared{id}) displayed in Figure~\ref{fig:timebound} corresponds to the relative difference between the execution time of $\rosa$ (resp.~$\fptaylor$) and $\realtofloat$, i.e.~the ratio $\frac{t_{\rosa} - t_{\realtofloat}}{t_{\realtofloat}}$ (resp.~$\frac{t_{\fptaylor} - t_{\realtofloat}}{t_{\realtofloat}}$). Similarly, the y-axis coordinate of the point \circled{id} (resp.~\squared{id}) is $\frac{\epsilon_{\rosa} - \epsilon_{\realtofloat}}{\epsilon_{\realtofloat}}$ (resp.~$\frac{\epsilon_{\fptaylor} - \epsilon_{\realtofloat}}{\epsilon_{\realtofloat}}$). For readability purpose, we used an appropriate logarithmic scale for the x-axis. 

The axes of the coordinate system of Figure~\ref{fig:timebound} divide the plane into four quadrants: 
the nonnegative quadrant $(+,+)$ contains data points referring to programs for which $\realtofloat$ computes the tighter bounds in less time, 
the second one $(+,-)$ contains points referring to programs for which $\realtofloat$ is slower but more accurate, 
the non-positive quadrant $(-, -)$ for which $\realtofloat$ is slower and computes coarser bounds 
and 
the last one $(-,+)$ for which $\realtofloat$ is faster but less accurate. On the quadrant $(+,-)$, one can see that $\realtofloat$ computes bound which are less accurate than $\fptaylor$ on programs \code{a-c} and \code{t}, but much faster for program \code{b}. The quadrant $(-, +)$ indicates that $\rosa$ (resp.~$\fptaylor$) is more efficient but less precise than $\realtofloat$ on program \code{d} (resp.~\code{o}). The presence of the majority of plots on the nonnegative quadrant $(+, +)$ confirms that $\realtofloat$ does not compromise efficiency at the expense of accuracy.
%

For each program implementing polynomials, our tool has an option to provide formal guarantees for the corresponding roundoff error bound $\epsilon_{\realtofloat}$. Using the formal mechanism described in Section~\ref{sec:coqbackground}, $\realtofloat$  formally checks inside $\coq$ the SOS certificates generated by the SDP solver. 

%
\begin{figure}[!ht]
\begin{center}
\includegraphics[scale=0.76]{formal.pdf}
\caption{Comparisons of informal and formal execution times to certify bounds of roundoff errors obtained with $\fptaylor$, relatively to $\realtofloat$}
\label{fig:formal}
\end{center}
\end{figure}
%

The $\fptaylor$ software has a similar option to provide formal scripts which can be checked inside the $\hol$ proof assistant, for programs involving polynomial and transcendental functions. Our tool has more limitations to perform formal verification as it cannot handle non-polynomial programs.
Next, we describe the formal proof results obtained while verifying the bounds for the eight polynomial programs \code{d-f} and \code{i-n}. 
Figure~\ref{fig:formal} allows to compare the execution times of $\realtofloat$ and  $\fptaylor$ required to analyze the eight programs in both informal (i.e. without verification inside $\coq$ or $\hol$) and formal settings. The x-axis coordinate of each point \squared{id} displayed in Figure~\ref{fig:timebound} is the same as on Figure~\ref{fig:timebound}. The dotted line represents the bisector of the first quadrant angle.
The y-axis coordinate of the point \squared{id} corresponds to the relative difference between the execution time $t_{\hol}$ to obtain a $\hol$ formal proof with $\fptaylor$ and the execution time $t_{\coq}$ to obtain a $\coq$ formal proof with $\realtofloat$, i.e.~the ratio $\frac{t_{\hol} - t_{\coq}}{t_{\coq}}$. 
Note that all points have nonnegative y-axis coordinates, as our tool formally checks all program bounds more efficiently than $\fptaylor$ (more than hundred times faster for the three programs \code{i}, \code{m-n}). The figure shows that the speedup ratio in the formal setting is higher than in the informal setting for the four programs \code{i} and \code{l-n}, as the corresponding points are above the first bisector. This emphasizes the benefit of using SOS certificates for formal verification rather than optimization methods based on Taylor approximations. The latter performs almost as well as the former to analyze program \code{k} but yields a coarser bound.
%
%However, $\fptaylor$ implements special cases to eliminate some error terms, for instance when $\op$ is the multiplication and one of the operands is a nonnegative power of two, then the error $e$ is set to zero. 
% We didn't succeed to find conditional programs with more than 2 variables and constraints of degree greater than 3. 
%}\fi
%\newpage
%\vspace*{-0.2cm}
\section{Related Works}
%
\paragraph{Satisfiability Modulo Theories (SMT)}
SMT solvers allow to analyze programs with various semantics or specifications but are limited for the manipulation of problems involving nonlinear arithmetic. 
Several solvers, including {\sc Z3}~\cite{DeMoura08}, provide partial support for the IEEE floating-point standard~\cite{smtFPA2010}. They suffer from a lack of scalability when used for roundoff error analysis in isolation (as emphasized in~\cite{Darulova14Popl}), but can be integrated into existing frameworks, e.g.~{\sc FPhile}~\cite{PaganelliA13}. The procedure in~\cite{dReal13} can solve SMT problems over the real numbers, using interval constraint propagation but has not yet been applied to quantification of roundoff error.
The $\rosa$ tool~\cite{Darulova14Popl} provides a way to compile real programs involving semialgebraic functions and conditional statements.
The tool uses affine arithmetic to provide sound over-approximations of roundoff errors, allowing for generation of finite precision implementations which fulfill the required precision given as input by the user. Bounds of the affine expressions are provided through an optimization procedure based on SMT. In our case, we use the same rounding model but provide approximations which are affine w.r.t. the additional error variables and nonlinear w.r.t. the input variables. Instead of using SMT, we bound the resulting expressions with optimization techniques based on semidefinite programming.
At the moment (and in contrast to our method) the $\rosa$ tool does not formally verify the output bound provided by the SMT solver, but such a feature could be embedded through an interface with the $\smtcoq$ framework~\cite{smtcoq}. This latter tool allows the proof witness generated by an SMT solver to be formally (and independently) re-checked inside $\coq$.  
The $\smtcoq$ framework uses tactics based on {\em computational reflection} to enable this re-checking to be performed efficiently.
%While ensuring soundness, the efficiency of this procedure is not compromised due to tactics enjoying the mechanism of computational reflection.
%
\paragraph{Abstract Domains}
Abstract interpretation~\cite{CousotCousot77} has been extensively used in the context of static analysis to provide sound over-approximations, called {\em abstractions}, of the sets of values the variables of a program may take at each program point. The effects of variable assignments, guards and conditional loops statements are handled with several domain specific operators (e.g. inclusion, meet and join). Well studied abstract domains include intervals~\cite{Moore62} as well as more complicated frameworks based on affine arithmetic~\cite{Stolfi03}, octogons~\cite{octogons}, zonotopes~\cite{Zonotope10}, polyhedra~\cite{polyhedra08}, interval polyhedra~\cite{IntervalPoly09}, some of them being implemented inside a tool called {\sc Apron}~\cite{Apron09}. Abstract domains provide sound over-approximations of program expressions, and allow upper bounds on roundoff error to be computed. 
The {\sc Gappa} tool~\cite{Daumas10} relies on interval abstract domains with an extension to affine domains~\cite{Linderman10}, to reason about roundoff errors.
As demonstrated in~\cite{fptaylor15}, the bounds obtained inside {\sc Gappa} are often coarser than other methods which take into account the variable correlations. Formal guarantees can be provided as {\sc Gappa} benefits from an interface with $\coq$ while making use of interval libraries~\cite{Melquiond201214} relying on formalized floating-points~\cite{BM11Flocq}. The static analysis tool {\sc Fluctuat} relies on affine abstract domains~\cite{Blanchet03}. {\sc Gappa} and {\sc Fluctuat} tools use a different rounding model (also available as an option inside $\fptaylor$) based on a piecewise constant relative error bound. This is more precise than our current rounding model but requires extensive use of a branch and bound algorithm as each interval has to be subdivided in intervals $[2^n, 2^{n+1}]$ for several values of the integer $n$.  As a side effect, the corresponding optimization procedure is computationally demanding, as noticed in~\cite{fptaylor15}.

\paragraph{Global Optimization Frameworks}
Computing sound bounds of nonlinear expressions is mandatory to perform formal analysis of finite precision implementations and can be performed with various optimization tools. 
In the polynomial case, alternative approaches to semidefinite relaxations are based on decomposition in the multivariate Bernstein basis. Formal verification of bounds obtained with this decomposition has been investigated in the thesis of Zumkeller~\cite{ZumkellerPhD} and by Mun\~oz and Narkawicz~\cite{MN13} in the PVS theorem prover~\cite{PVS}. We are not aware of any work based on these techniques which can quantify roundoff errors. Another decomposition of nonnegative polynomials into SOS certificates consists in using the Krivine~\cite{Krivine1964b}-Handelman~\cite{Handelman1988} representation and boils down to solving linear programming (LP) relaxations. In our case, we use a different representation, leading to solve SDP relaxations. The Krivine-Handelman representation has been used in~\cite{Boland10HGR} to compute roundoff error bounds. LP relaxations often provide coarser bounds than SDP relaxations and it has been proven in~\cite{lasserre2009moments} that generically finite convergence does not occur for convex problems, at the exception of the linear case. 
Branch and bound methods with Taylor models~\cite{Berz09} are not restricted to polynomial systems and have been formalized~\cite{SolovyevH13} to solve nonlinear inequalities occurring in the proof of Kepler Conjecture. Symbolic Taylor Expansions~\cite{fptaylor15} have been implemented in the $\fptaylor$ tool to compute formal bounds of roundoff errors for programs involving both polynomial and transcendental functions. This method happens to be efficient and precise to analyze various programs and it would be interesting to design a procedure combining $\fptaylor$ with our tool on specific subsets of input constraints.
%
%\vspace*{-0.3cm}
\section{Conclusion and Perspectives} %0.5p
%
Our verification framework allows us to over-approximate roundoff errors occurring while executing nonlinear programs implemented with finite precision.
The framework relies on semidefinite optimization, ensuring certified approximations. Our approach extends to medium-size nonlinear problems, due to  automatic detection of the correlation sparsity pattern of input variables and round-off error variables. 
Experimental results indicate that the optimization algorithm implemented in our $\realtofloat$ software package often produces tighter error bounds than the ones provided by the competitive solvers $\rosa$ and $\fptaylor$.  In addition, $\realtofloat$ produces sums of squares certificates which guarantee the correctness of these upper bounds and can can be efficiently verified inside the $\coq$ proof assistant.

This work yields several directions for further research investigation. 
First, we intend to increase the size of graspable instances by exploiting symmetry patterns of certain program sub-classes to use specific SDP hierarchies~\cite{Riener2013SymmetricSDP}. We could also provide roundoff error bounds for more general programs, involving either finite or infinite conditional loops. A preliminary mandatory step is to be able to generate inductive invariants with SDP relaxations. Another interesting direction would be to apply the method used in~\cite{Lasserre11} to derive sequences of lower roundoff error bounds together with SDP-based certificates.
On the formal proof side, we could benefit from floating-point/interval arithmetic libraries available inside $\coq$, first to improve the efficiency of the formal polynomial checker, currently relying on exact arithmetic, then to extend the formal verification to non-polynomial programs.
%
%to improve the efficiency of the formal polynomial checker inside Coq by using the interval libraries available inside the proof assistant, rather than using exact polynomial arithmetic as in the current framework. We also plan to use these interval libraries extend our formal verification framework to the non-polynomial programs.
%Such extensions have become realistic as recent works allow to approximate inductive invariants with semidefinite relaxations.
Finally, we plan to combine this optimization framework with the procedure in~\cite{Gao15FPGA} to improve the automatic reordering of arithmetic expressions, allowing more efficient optimization of FPGA implementations.

\acks
This work was partly funded by the Engineering and Physical Sciences Research Council (EPSRC) Challenging Engineering Grant (EP/I020457/1).
% Thank John Wickerson, Alexey Solovyev, Eva Darulova
% We recommend abbrvnat bibliography style.
%\newpage
%~
\newpage
\bibliographystyle{abbrvnat}
\bibliography{roundsdp}
% The bibliography should be embedded for final submission.

\if{
\begin{thebibliography}{}
\softraggedright

\bibitem[Smith et~al.(2009)Smith, Jones]{smith02}
P. Q. Smith, and X. Y. Jones. ...reference text...

\end{thebibliography}
}\fi

%\newpage
%~
%\newpage
%\appendix
%\section*{Appendix A: Nonlinear Programs}
%\label{sec:appa}
%
%\input{appendix}

\end{document}

%                       Revision History
%                       -------- -------
%  Date         Person  Ver.    Change
%  ----         ------  ----    ------

%  2013.06.29   TU      0.1--4  comments on permission/copyright notices

